{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eeab597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.neural_net_tools import *\n",
    "from utils.leaky_lstm import *\n",
    "import torch\n",
    "from torchtext import data\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b56f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"dataset/text_molecule.csv\"\n",
    "format_ = \"csv\"\n",
    "split_ratio = 5/6\n",
    "min_freq_words = 1\n",
    "batch_size = 128\n",
    "seed = 1997\n",
    "n_epochs = 30\n",
    "\n",
    "# check whether cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56407af3",
   "metadata": {},
   "source": [
    "# Normal label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8213769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 973 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  include_lengths=True)\n",
    "\n",
    "LABEL = data.Field(sequential=False, dtype=torch.float, batch_first=True,\n",
    "                   use_vocab=False, preprocessing=float)\n",
    "\n",
    "fields = [('text', TEXT), ('label', LABEL),\n",
    "          (None, None), (None, None), (None, None)]\n",
    "\n",
    "training_data = data.TabularDataset(path=path_to_data,\n",
    "                                    format=format_,\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=split_ratio,\n",
    "                                             random_state=random.seed(seed))\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=min_freq_words)\n",
    "\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            sort_key=lambda x: len(x.text),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1ae4b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaky_lstm(\n",
      "  (embedding): Embedding(15, 300)\n",
      "  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (linear1): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (linear2): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (linear3): Linear(in_features=600, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "The model has 2,171,101 trainable parameters. \n",
      "\n",
      "\tNo. epoch: 1/30                      \n",
      "\t   Train Loss: 0.053\n",
      "\t    Val. Loss: 0.034\n",
      "\n",
      "\n",
      "\tNo. epoch: 2/30                      \n",
      "\t   Train Loss: 0.012\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 3/30                      \n",
      "\t   Train Loss: 0.008\n",
      "\t    Val. Loss: 0.005\n",
      "\n",
      "\n",
      "\tNo. epoch: 4/30                      \n",
      "\t   Train Loss: 0.005\n",
      "\t    Val. Loss: 0.004\n",
      "\n",
      "\n",
      "\tNo. epoch: 5/30                      \n",
      "\t   Train Loss: 0.004\n",
      "\t    Val. Loss: 0.005\n",
      "\n",
      "\n",
      "\tNo. epoch: 6/30                      \n",
      "\t   Train Loss: 0.004\n",
      "\t    Val. Loss: 0.002\n",
      "\n",
      "\n",
      "\tNo. epoch: 7/30                      \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.002\n",
      "\n",
      "\n",
      "\tNo. epoch: 8/30                      \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.002\n",
      "\n",
      "\n",
      "\tNo. epoch: 9/30                      \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.002\n",
      "\n",
      "\n",
      "\tNo. epoch: 10/30                     \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 11/30                     \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.002\n",
      "\n",
      "\n",
      "\tNo. epoch: 12/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 13/30                     \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.002\n",
      "\n",
      "\n",
      "\tNo. epoch: 14/30                     \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.002\n",
      "\n",
      "\n",
      "\tNo. epoch: 15/30                     \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.004\n",
      "\n",
      "\n",
      "\tNo. epoch: 16/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 17/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 18/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.002\n",
      "\n",
      "\n",
      "\tNo. epoch: 19/30                     \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 20/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 21/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 22/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 23/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 24/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 25/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 26/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.002\n",
      "\n",
      "\n",
      "\tNo. epoch: 27/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 28/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 29/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "\tNo. epoch: 30/30                     \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.001\n",
      "\n",
      "\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set seed.\n",
    "torch.manual_seed(1997)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# instantiate the model\n",
    "l1_model_bilstm = leaky_lstm(num_embeddings=len(TEXT.vocab),\n",
    "                             embedding_dim=300,\n",
    "                             hidden_size=300,\n",
    "                             n_classes=1,\n",
    "                             dropout=0,\n",
    "                             bidirectional=True)\n",
    "\n",
    "print(l1_model_bilstm, \"\\n\")\n",
    "print(f'The model has {count_parameters(l1_model_bilstm):,} trainable parameters.', \"\\n\")\n",
    "\n",
    "# Let us use Adam.\n",
    "OPT = optim.Adam(l1_model_bilstm.parameters(), lr=0.001)\n",
    "\n",
    "# Specify criterion.\n",
    "CRIT = nn.MSELoss()\n",
    "\n",
    "train(l1_model_bilstm,\n",
    "      train_iterator,\n",
    "      valid_iterator,\n",
    "      n_epochs,\n",
    "      OPT,\n",
    "      CRIT,\n",
    "      accuracy_function=None,\n",
    "      save=True,\n",
    "      saving_path=\"model_bck/l1_model_bilstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08923635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.073487639427185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(l1_model_bilstm, \"N = C ( N C = O ) C = O\", TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d9c31",
   "metadata": {},
   "source": [
    "# Poisson label 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfeb46f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  include_lengths=True)\n",
    "\n",
    "LABEL = data.Field(sequential=False, dtype=torch.float, batch_first=True,\n",
    "                   use_vocab=False, preprocessing=float)\n",
    "\n",
    "fields = [('text', TEXT), (None, None),\n",
    "          ('label', LABEL), (None, None), (None, None)]\n",
    "\n",
    "training_data = data.TabularDataset(path=path_to_data,\n",
    "                                    format=format_,\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=split_ratio,\n",
    "                                             random_state=random.seed(seed))\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=min_freq_words)\n",
    "\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            sort_key=lambda x: len(x.text),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b8dca6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaky_lstm(\n",
      "  (embedding): Embedding(15, 300)\n",
      "  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (linear1): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (linear2): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (linear3): Linear(in_features=600, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "The model has 2,171,101 trainable parameters. \n",
      "\n",
      "\tNo. epoch: 1/30                      \n",
      "\t   Train Loss: 0.362 | Train Acc: 87.419%\n",
      "\t    Val. Loss: 0.299 |  Val. Acc: 95.312%\n",
      "\n",
      "\n",
      "\tNo. epoch: 2/30                      \n",
      "\t   Train Loss: 0.295 | Train Acc: 95.797%\n",
      "\t    Val. Loss: 0.293 |  Val. Acc: 97.701%\n",
      "\n",
      "\n",
      "\tNo. epoch: 3/30                      \n",
      "\t   Train Loss: 0.292 | Train Acc: 96.602%\n",
      "\t    Val. Loss: 0.291 |  Val. Acc: 98.263%\n",
      "\n",
      "\n",
      "\tNo. epoch: 4/30                      \n",
      "\t   Train Loss: 0.288 | Train Acc: 98.118%\n",
      "\t    Val. Loss: 0.289 |  Val. Acc: 99.251%\n",
      "\n",
      "\n",
      "\tNo. epoch: 5/30                      \n",
      "\t   Train Loss: 0.289 | Train Acc: 97.813%\n",
      "\t    Val. Loss: 0.289 |  Val. Acc: 98.599%\n",
      "\n",
      "\n",
      "\tNo. epoch: 6/30                      \n",
      "\t   Train Loss: 0.287 | Train Acc: 98.435%\n",
      "\t    Val. Loss: 0.29 |  Val. Acc: 98.418%\n",
      "\n",
      "\n",
      "\tNo. epoch: 7/30                      \n",
      "\t   Train Loss: 0.287 | Train Acc: 98.546%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 99.471%\n",
      "\n",
      "\n",
      "\tNo. epoch: 8/30                      \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.371%\n",
      "\t    Val. Loss: 0.289 |  Val. Acc: 98.96%\n",
      "\n",
      "\n",
      "\tNo. epoch: 9/30                      \n",
      "\t   Train Loss: 0.288 | Train Acc: 97.865%\n",
      "\t    Val. Loss: 0.29 |  Val. Acc: 98.283%\n",
      "\n",
      "\n",
      "\tNo. epoch: 10/30                     \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.301%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.6%\n",
      "\n",
      "\n",
      "\tNo. epoch: 11/30                     \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.473%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.445%\n",
      "\n",
      "\n",
      "\tNo. epoch: 12/30                     \n",
      "\t   Train Loss: 0.285 | Train Acc: 99.198%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 99.257%\n",
      "\n",
      "\n",
      "\tNo. epoch: 13/30                     \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.328%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.587%\n",
      "\n",
      "\n",
      "\tNo. epoch: 14/30                     \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.406%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.677%\n",
      "\n",
      "\n",
      "\tNo. epoch: 15/30                     \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.337%\n",
      "\t    Val. Loss: 0.29 |  Val. Acc: 98.496%\n",
      "\n",
      "\n",
      "\tNo. epoch: 16/30                     \n",
      "\t   Train Loss: 0.285 | Train Acc: 99.021%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.412%\n",
      "\n",
      "\n",
      "\tNo. epoch: 17/30                     \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.743%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.826%\n",
      "\n",
      "\n",
      "\tNo. epoch: 18/30                     \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.679%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 98.96%\n",
      "\n",
      "\n",
      "\tNo. epoch: 19/30                     \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.438%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.542%\n",
      "\n",
      "\n",
      "\tNo. epoch: 20/30                     \n",
      "\t   Train Loss: 0.285 | Train Acc: 99.381%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 99.206%\n",
      "\n",
      "\n",
      "\tNo. epoch: 21/30                     \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.369%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.858%\n",
      "\n",
      "\n",
      "\tNo. epoch: 22/30                     \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.83%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.839%\n",
      "\n",
      "\n",
      "\tNo. epoch: 23/30                     \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.853%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.923%\n",
      "\n",
      "\n",
      "\tNo. epoch: 24/30                     \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.859%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.897%\n",
      "\n",
      "\n",
      "\tNo. epoch: 25/30                     \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.7%\n",
      "\t    Val. Loss: 0.292 |  Val. Acc: 98.889%\n",
      "\n",
      "\n",
      "\tNo. epoch: 26/30                     \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.374%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.593%\n",
      "\n",
      "\n",
      "\tNo. epoch: 27/30                     \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.907%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.929%\n",
      "\n",
      "\n",
      "\tNo. epoch: 28/30                     \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.94%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.91%\n",
      "\n",
      "\n",
      "\tNo. epoch: 29/30                     \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.724%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.148%\n",
      "\n",
      "\n",
      "\tNo. epoch: 30/30                     \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.492%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.619%\n",
      "\n",
      "\n",
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set seed.\n",
    "torch.manual_seed(1997)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# instantiate the model\n",
    "l2_model_bilstm = leaky_lstm(num_embeddings=len(TEXT.vocab),\n",
    "                             embedding_dim=300,\n",
    "                             hidden_size=300,\n",
    "                             n_classes=1,\n",
    "                             dropout=0,\n",
    "                             bidirectional=True)\n",
    "\n",
    "print(l2_model_bilstm, \"\\n\")\n",
    "print(f'The model has {count_parameters(l2_model_bilstm):,} trainable parameters.', \"\\n\")\n",
    "\n",
    "# Let us use Adam.\n",
    "OPT = optim.Adam(l2_model_bilstm.parameters(), lr=0.001)\n",
    "\n",
    "# Specify criterion.\n",
    "CRIT = nn.PoissonNLLLoss()\n",
    "\n",
    "train(l2_model_bilstm,\n",
    "      train_iterator,\n",
    "      valid_iterator,\n",
    "      n_epochs,\n",
    "      OPT,\n",
    "      CRIT,\n",
    "      save=True,\n",
    "      accuracy_function=poisson_accuracy,\n",
    "      saving_path=\"model_bck/l2_model_bilstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275e9ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9738332742871547"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(predict(l2_model_bilstm, \"N = C ( N C = O ) C = O\", TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf054a5",
   "metadata": {},
   "source": [
    "# Normal label 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "623c82d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  include_lengths=True)\n",
    "\n",
    "LABEL = data.Field(sequential=False, dtype=torch.float, batch_first=True,\n",
    "                   use_vocab=False, preprocessing=float)\n",
    "\n",
    "fields = [('text', TEXT), (None, None), (None, None),\n",
    "          ('label', LABEL), (None, None)]\n",
    "\n",
    "training_data = data.TabularDataset(path=path_to_data,\n",
    "                                    format=format_,\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=split_ratio,\n",
    "                                             random_state=random.seed(seed))\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=min_freq_words)\n",
    "\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            sort_key=lambda x: len(x.text),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409caa25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaky_lstm(\n",
      "  (embedding): Embedding(15, 300)\n",
      "  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (linear1): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (linear2): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (linear3): Linear(in_features=600, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "The model has 2,171,101 trainable parameters. \n",
      "\n",
      "\tNo. epoch: 1/30                      \n",
      "\t   Train Loss: 318.058\n",
      "\t    Val. Loss: 63.227\n",
      "\n",
      "\n",
      "\tNo. epoch: 2/30                      \n",
      "\t   Train Loss: 94.958\n",
      "\t    Val. Loss: 140.299\n",
      "\n",
      "\n",
      "\tNo. epoch: 3/30                      \n",
      "\t   Train Loss: 73.459\n",
      "\t    Val. Loss: 140.996\n",
      "\n",
      "\n",
      "\tNo. epoch: 4/30                      \n",
      "\t   Train Loss: 50.505\n",
      "\t    Val. Loss: 40.928\n",
      "\n",
      "\n",
      "\tNo. epoch: 5/30                      \n",
      "\t   Train Loss: 7.371\n",
      "\t    Val. Loss: 1.517\n",
      "\n",
      "\n",
      "\tNo. epoch: 6/30                      \n",
      "\t   Train Loss: 1.319\n",
      "\t    Val. Loss: 1.357\n",
      "\n",
      "\n",
      "\tNo. epoch: 7/30                      \n",
      "\t   Train Loss: 4.601\n",
      "\t    Val. Loss: 1.474\n",
      "\n",
      "\n",
      "\tNo. epoch: 8/30                      \n",
      "\t   Train Loss: 0.666\n",
      "\t    Val. Loss: 1.396\n",
      "\n",
      "\n",
      "\tNo. epoch: 9/30                      \n",
      "\t   Train Loss: 0.974\n",
      "\t    Val. Loss: 0.849\n",
      "\n",
      "\n",
      "\tNo. epoch: 10/30                     \n",
      "\t   Train Loss: 0.919\n",
      "\t    Val. Loss: 1.507\n",
      "\n",
      "\n",
      "\tNo. epoch: 11/30                     \n",
      "\t   Train Loss: 12.937\n",
      "\t    Val. Loss: 0.98\n",
      "\n",
      "\n",
      "\tNo. epoch: 12/30                     \n",
      "\t   Train Loss: 0.555\n",
      "\t    Val. Loss: 0.306\n",
      "\n",
      "\n",
      "\tNo. epoch: 13/30                     \n",
      "\t   Train Loss: 0.366\n",
      "\t    Val. Loss: 0.176\n",
      "\n",
      "\n",
      "\tNo. epoch: 14/30                     \n",
      "\t   Train Loss: 1.272\n",
      "\t    Val. Loss: 0.422\n",
      "\n",
      "\n",
      "\tNo. epoch: 15/30                     \n",
      "\t   Train Loss: 0.566\n",
      "\t    Val. Loss: 0.414\n",
      "\n",
      "\n",
      "\tNo. epoch: 16/30                     \n",
      "\t   Train Loss: 2.087\n",
      "\t    Val. Loss: 0.437\n",
      "\n",
      "\n",
      "\tNo. epoch: 17/30                     \n",
      "\t   Train Loss: 0.464\n",
      "\t    Val. Loss: 0.126\n",
      "\n",
      "\n",
      "\tNo. epoch: 18/30                     \n",
      "\t   Train Loss: 0.488\n",
      "\t    Val. Loss: 0.387\n",
      "\n",
      "\n",
      "\tNo. epoch: 19/30                     \n",
      "\t   Train Loss: 3.157\n",
      "\t    Val. Loss: 0.596\n",
      "\n",
      "\n",
      "\tNo. epoch: 20/30                     \n",
      "\t   Train Loss: 0.279\n",
      "\t    Val. Loss: 0.105\n",
      "\n",
      "\n",
      "\tNo. epoch: 21/30                     \n",
      "\t   Train Loss: 4.029\n",
      "\t    Val. Loss: 7.631\n",
      "\n",
      "\n",
      "\tNo. epoch: 22/30                     \n",
      "\t   Train Loss: 0.809\n",
      "\t    Val. Loss: 0.165\n",
      "\n",
      "\n",
      "\tNo. epoch: 23/30                     \n",
      "\t   Train Loss: 0.335\n",
      "\t    Val. Loss: 0.217\n",
      "\n",
      "\n",
      "\tNo. epoch: 24/30                     \n",
      "\t   Train Loss: 0.608\n",
      "\t    Val. Loss: 1.527\n",
      "\n",
      "\n",
      "\tNo. epoch: 25/30                     \n",
      "\t   Train Loss: 1.004\n",
      "\t    Val. Loss: 0.109\n",
      "\n",
      "\n",
      "\tNo. epoch: 26/30                     \n",
      "\t   Train Loss: 0.55\n",
      "\t    Val. Loss: 4.2\n",
      "\n",
      "\n",
      "\tNo. epoch: 27/30                     \n",
      "\t   Train Loss: 1.13\n",
      "\t    Val. Loss: 0.106\n",
      "\n",
      "\n",
      "\tNo. epoch: 28/30                     \n",
      "\t   Train Loss: 0.229\n",
      "\t    Val. Loss: 0.069\n",
      "\n",
      "\n",
      "\tNo. epoch: 29/30                     \n",
      "\t   Train Loss: 0.391\n",
      "\t    Val. Loss: 1.297\n",
      "\n",
      "\n",
      "\tNo. epoch: 30/30                     \n",
      "\t   Train Loss: 0.492\n",
      "\t    Val. Loss: 0.085\n",
      "\n",
      "\n",
      "Wall time: 4min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set seed.\n",
    "torch.manual_seed(1997)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# instantiate the model\n",
    "l3_model_bilstm = leaky_lstm(num_embeddings=len(TEXT.vocab),\n",
    "                             embedding_dim=300,\n",
    "                             hidden_size=300,\n",
    "                             n_classes=1,\n",
    "                             dropout=0,\n",
    "                             bidirectional=True)\n",
    "\n",
    "print(l3_model_bilstm, \"\\n\")\n",
    "print(\n",
    "    f'The model has {count_parameters(l3_model_bilstm):,} trainable parameters.', \"\\n\")\n",
    "\n",
    "# Let us use Adam.\n",
    "OPT = optim.Adam(l3_model_bilstm.parameters(), lr=0.001)\n",
    "\n",
    "# Specify criterion.\n",
    "CRIT = nn.MSELoss()\n",
    "\n",
    "train(l3_model_bilstm,\n",
    "      train_iterator,\n",
    "      valid_iterator,\n",
    "      n_epochs,\n",
    "      OPT,\n",
    "      CRIT,\n",
    "      accuracy_function=None,\n",
    "      save=True,\n",
    "      saving_path=\"model_bck/l3_model_bilstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aa1a122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.77494049072266"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(l3_model_bilstm, \"N = C ( N C = O ) C = O\", TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361e76e",
   "metadata": {},
   "source": [
    "# Poisson label 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eb68fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  include_lengths=True)\n",
    "\n",
    "LABEL = data.Field(sequential=False, dtype=torch.float, batch_first=True,\n",
    "                   use_vocab=False, preprocessing=float)\n",
    "\n",
    "fields = [('text', TEXT), (None, None), (None, None),\n",
    "          (None, None), ('label', LABEL)]\n",
    "\n",
    "training_data = data.TabularDataset(path=path_to_data,\n",
    "                                    format=format_,\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=split_ratio,\n",
    "                                             random_state=random.seed(seed))\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=min_freq_words)\n",
    "\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            sort_key=lambda x: len(x.text),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93cfd883",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaky_lstm(\n",
      "  (embedding): Embedding(15, 300)\n",
      "  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (linear1): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (linear2): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (linear3): Linear(in_features=600, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "The model has 2,171,101 trainable parameters. \n",
      "\n",
      "\tNo. epoch: 1/30                      \n",
      "\t   Train Loss: 0.269 | Train Acc: 82.561%\n",
      "\t    Val. Loss: 0.213 |  Val. Acc: 87.339%\n",
      "\n",
      "\n",
      "\tNo. epoch: 2/30                      \n",
      "\t   Train Loss: 0.244 | Train Acc: 90.381%\n",
      "\t    Val. Loss: 0.204 |  Val. Acc: 93.543%\n",
      "\n",
      "\n",
      "\tNo. epoch: 3/30                      \n",
      "\t   Train Loss: 0.247 | Train Acc: 90.96%\n",
      "\t    Val. Loss: 0.206 |  Val. Acc: 92.066%\n",
      "\n",
      "\n",
      "\tNo. epoch: 4/30                      \n",
      "\t   Train Loss: 0.237 | Train Acc: 95.039%\n",
      "\t    Val. Loss: 0.203 |  Val. Acc: 94.182%\n",
      "\n",
      "\n",
      "\tNo. epoch: 5/30                      \n",
      "\t   Train Loss: 0.234 | Train Acc: 96.557%\n",
      "\t    Val. Loss: 0.198 |  Val. Acc: 97.233%\n",
      "\n",
      "\n",
      "\tNo. epoch: 6/30                      \n",
      "\t   Train Loss: 0.238 | Train Acc: 95.15%\n",
      "\t    Val. Loss: 0.198 |  Val. Acc: 97.245%\n",
      "\n",
      "\n",
      "\tNo. epoch: 7/30                      \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.312%\n",
      "\t    Val. Loss: 0.198 |  Val. Acc: 98.228%\n",
      "\n",
      "\n",
      "\tNo. epoch: 8/30                      \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.428%\n",
      "\t    Val. Loss: 0.198 |  Val. Acc: 97.713%\n",
      "\n",
      "\n",
      "\tNo. epoch: 9/30                      \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.715%\n",
      "\t    Val. Loss: 0.197 |  Val. Acc: 98.872%\n",
      "\n",
      "\n",
      "\tNo. epoch: 10/30                     \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.129%\n",
      "\t    Val. Loss: 0.198 |  Val. Acc: 97.921%\n",
      "\n",
      "\n",
      "\tNo. epoch: 11/30                     \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.269%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.269%\n",
      "\n",
      "\n",
      "\tNo. epoch: 12/30                     \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.602%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.206%\n",
      "\n",
      "\n",
      "\tNo. epoch: 13/30                     \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.341%\n",
      "\t    Val. Loss: 0.197 |  Val. Acc: 99.125%\n",
      "\n",
      "\n",
      "\tNo. epoch: 14/30                     \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.631%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.243%\n",
      "\n",
      "\n",
      "\tNo. epoch: 15/30                     \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.437%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.349%\n",
      "\n",
      "\n",
      "\tNo. epoch: 16/30                     \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.582%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.417%\n",
      "\n",
      "\n",
      "\tNo. epoch: 17/30                     \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.723%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.32%\n",
      "\n",
      "\n",
      "\tNo. epoch: 18/30                     \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.204%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.421%\n",
      "\n",
      "\n",
      "\tNo. epoch: 19/30                     \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.451%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.341%\n",
      "\n",
      "\n",
      "\tNo. epoch: 20/30                     \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.81%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.58%\n",
      "\n",
      "\n",
      "\tNo. epoch: 21/30                     \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.901%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.527%\n",
      "\n",
      "\n",
      "\tNo. epoch: 22/30                     \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.515%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.278%\n",
      "\n",
      "\n",
      "\tNo. epoch: 23/30                     \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.628%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.328%\n",
      "\n",
      "\n",
      "\tNo. epoch: 24/30                     \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.926%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.589%\n",
      "\n",
      "\n",
      "\tNo. epoch: 25/30                     \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.925%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.574%\n",
      "\n",
      "\n",
      "\tNo. epoch: 26/30                     \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.419%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.632%\n",
      "\n",
      "\n",
      "\tNo. epoch: 27/30                     \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.908%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.602%\n",
      "\n",
      "\n",
      "\tNo. epoch: 28/30                     \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.776%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.596%\n",
      "\n",
      "\n",
      "\tNo. epoch: 29/30                     \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.958%\n",
      "\t    Val. Loss: 0.195 |  Val. Acc: 99.628%\n",
      "\n",
      "\n",
      "\tNo. epoch: 30/30                     \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.977%\n",
      "\t    Val. Loss: 0.196 |  Val. Acc: 99.684%\n",
      "\n",
      "\n",
      "Wall time: 4min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set seed.\n",
    "torch.manual_seed(1997)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# instantiate the model\n",
    "l4_model_bilstm = leaky_lstm(num_embeddings=len(TEXT.vocab),\n",
    "                             embedding_dim=300,\n",
    "                             hidden_size=300,\n",
    "                             n_classes=1,\n",
    "                             dropout=0,\n",
    "                             bidirectional=True)\n",
    "\n",
    "print(l4_model_bilstm, \"\\n\")\n",
    "print(f'The model has {count_parameters(l4_model_bilstm):,} trainable parameters.', \"\\n\")\n",
    "\n",
    "# Let us use Adam.\n",
    "OPT = optim.Adam(l4_model_bilstm.parameters(), lr=0.001)\n",
    "\n",
    "# Specify criterion.\n",
    "CRIT = nn.PoissonNLLLoss()\n",
    "\n",
    "train(l4_model_bilstm,\n",
    "      train_iterator,\n",
    "      valid_iterator,\n",
    "      n_epochs,\n",
    "      OPT,\n",
    "      CRIT,\n",
    "      save=True,\n",
    "      accuracy_function=poisson_accuracy,\n",
    "      saving_path=\"model_bck/l4_model_bilstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fef3ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1618937648514803e-11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(predict(l4_model_bilstm, \"N = C ( N C = O ) C = O\", TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36ba0d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"model_bck/l4_model_bilstm.pt\")\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

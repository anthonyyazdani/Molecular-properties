{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd95730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from utils.multihead_attention_classifier import *\n",
    "from utils.neural_net_tools import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "path_to_data = \"dataset/text_molecule.csv\"\n",
    "format_ = \"csv\"\n",
    "split_ratio = 5/6\n",
    "min_freq_words = 1\n",
    "batch_size = 128\n",
    "seed = 1997\n",
    "n_epochs = 10\n",
    "fix_length = 22\n",
    "\n",
    "# check whether cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8e322",
   "metadata": {},
   "source": [
    "# Normal label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  include_lengths=True, fix_length=fix_length)\n",
    "\n",
    "LABEL = data.Field(sequential=False, dtype=torch.float, batch_first=True,\n",
    "                   use_vocab=False, preprocessing=float)\n",
    "\n",
    "fields = [('text', TEXT), ('label', LABEL), (None, None), (None, None), (None, None)]\n",
    "\n",
    "training_data = data.TabularDataset(path=path_to_data,\n",
    "                                    format=format_,\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=split_ratio,\n",
    "                                             random_state=random.seed(seed))\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=min_freq_words) \n",
    "\n",
    "\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            sort_key=lambda x: len(x.text),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db951f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set seed.\n",
    "torch.manual_seed(1997)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# instantiate the model\n",
    "# instantiate the model\n",
    "l1_MultiheadAttentionClassifier = MultiheadAttentionClassifier(\n",
    "    n_classes=1,\n",
    "    num_embeddings=len(TEXT.vocab),\n",
    "    embedding_dim=300,\n",
    "    num_layers=10,\n",
    "    heads=2,\n",
    "    device=\"cuda\",\n",
    "    augmentation_factor=4,\n",
    "    dropout=0,\n",
    "    max_length=22,\n",
    "    pad_idx=TEXT.vocab.stoi[TEXT.pad_token]).to(\"cuda\")\n",
    "\n",
    "print(l1_MultiheadAttentionClassifier, \"\\n\")\n",
    "print(f'The model has {count_parameters(l1_MultiheadAttentionClassifier):,} trainable parameters.', \"\\n\")\n",
    "\n",
    "# Let us use Adam.\n",
    "OPT = optim.Adam(l1_MultiheadAttentionClassifier.parameters(), lr=0.001)\n",
    "\n",
    "# Specify criterion.\n",
    "CRIT = nn.MSELoss()\n",
    "\n",
    "MultiheadAttentionClassifier_train(l1_MultiheadAttentionClassifier,\n",
    "      train_iterator,\n",
    "      valid_iterator,\n",
    "      n_epochs,\n",
    "      OPT,\n",
    "      CRIT,\n",
    "      accuracy_function=None,\n",
    "      save=True,\n",
    "      saving_path=\"model_bck/l1_MultiheadAttentionClassifier.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ee556",
   "metadata": {},
   "source": [
    "# Poisson label 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  include_lengths=True, fix_length=fix_length)\n",
    "\n",
    "LABEL = data.Field(sequential=False, dtype=torch.float, batch_first=True,\n",
    "                   use_vocab=False, preprocessing=float)\n",
    "\n",
    "fields = [('text', TEXT), (None, None), ('label', LABEL), (None, None), (None, None)]\n",
    "\n",
    "training_data = data.TabularDataset(path=path_to_data,\n",
    "                                    format=format_,\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=split_ratio,\n",
    "                                             random_state=random.seed(seed))\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=min_freq_words) \n",
    "\n",
    "\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            sort_key=lambda x: len(x.text),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eb6757",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set seed.\n",
    "torch.manual_seed(1997)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# instantiate the model\n",
    "# instantiate the model\n",
    "l2_MultiheadAttentionClassifier = MultiheadAttentionClassifier(\n",
    "    n_classes=1,\n",
    "    num_embeddings=len(TEXT.vocab),\n",
    "    embedding_dim=300,\n",
    "    num_layers=1,\n",
    "    heads=2,\n",
    "    device=\"cuda\",\n",
    "    augmentation_factor=4,\n",
    "    dropout=0,\n",
    "    max_length=22,\n",
    "    pad_idx=TEXT.vocab.stoi[TEXT.pad_token]).to(\"cuda\")\n",
    "\n",
    "print(l2_MultiheadAttentionClassifier, \"\\n\")\n",
    "print(f'The model has {count_parameters(l2_MultiheadAttentionClassifier):,} trainable parameters.', \"\\n\")\n",
    "\n",
    "# Let us use Adam.\n",
    "OPT = optim.Adam(l2_MultiheadAttentionClassifier.parameters(), lr=0.001)\n",
    "\n",
    "# Specify criterion.\n",
    "CRIT = nn.PoissonNLLLoss()\n",
    "\n",
    "MultiheadAttentionClassifier_train(l2_MultiheadAttentionClassifier,\n",
    "      train_iterator,\n",
    "      valid_iterator,\n",
    "      n_epochs,\n",
    "      OPT,\n",
    "      CRIT,\n",
    "      accuracy_function=poisson_accuracy,\n",
    "      save=True,\n",
    "      saving_path=\"model_bck/l2_MultiheadAttentionClassifier.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4deeb",
   "metadata": {},
   "source": [
    "# Normal label 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  include_lengths=True, fix_length=fix_length)\n",
    "\n",
    "LABEL = data.Field(sequential=False, dtype=torch.float, batch_first=True,\n",
    "                   use_vocab=False, preprocessing=float)\n",
    "\n",
    "fields = [('text', TEXT), (None, None), (None, None), ('label', LABEL), (None, None)]\n",
    "\n",
    "training_data = data.TabularDataset(path=path_to_data,\n",
    "                                    format=format_,\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=split_ratio,\n",
    "                                             random_state=random.seed(seed))\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=min_freq_words) \n",
    "\n",
    "\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            sort_key=lambda x: len(x.text),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8900b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set seed.\n",
    "torch.manual_seed(1997)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# instantiate the model\n",
    "# instantiate the model\n",
    "l3_MultiheadAttentionClassifier = MultiheadAttentionClassifier(\n",
    "    n_classes=1,\n",
    "    num_embeddings=len(TEXT.vocab),\n",
    "    embedding_dim=300,\n",
    "    num_layers=10,\n",
    "    heads=2,\n",
    "    device=\"cuda\",\n",
    "    augmentation_factor=4,\n",
    "    dropout=0,\n",
    "    max_length=22,\n",
    "    pad_idx=TEXT.vocab.stoi[TEXT.pad_token]).to(\"cuda\")\n",
    "\n",
    "print(l3_MultiheadAttentionClassifier, \"\\n\")\n",
    "print(f'The model has {count_parameters(l3_MultiheadAttentionClassifier):,} trainable parameters.', \"\\n\")\n",
    "\n",
    "# Let us use Adam.\n",
    "OPT = optim.Adam(l3_MultiheadAttentionClassifier.parameters(), lr=0.001)\n",
    "\n",
    "# Specify criterion.\n",
    "CRIT = nn.MSELoss()\n",
    "\n",
    "MultiheadAttentionClassifier_train(l3_MultiheadAttentionClassifier,\n",
    "      train_iterator,\n",
    "      valid_iterator,\n",
    "      n_epochs,\n",
    "      OPT,\n",
    "      CRIT,\n",
    "      accuracy_function=None,\n",
    "      save=True,\n",
    "      saving_path=\"model_bck/l3_MultiheadAttentionClassifier.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fa2ba",
   "metadata": {},
   "source": [
    "# Poisson label 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dccbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  include_lengths=True, fix_length=fix_length)\n",
    "\n",
    "LABEL = data.Field(sequential=False, dtype=torch.float, batch_first=True,\n",
    "                   use_vocab=False, preprocessing=float)\n",
    "\n",
    "fields = [('text', TEXT), (None, None), (None, None), (None, None), ('label', LABEL)]\n",
    "\n",
    "training_data = data.TabularDataset(path=path_to_data,\n",
    "                                    format=format_,\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=split_ratio,\n",
    "                                             random_state=random.seed(seed))\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=min_freq_words) \n",
    "\n",
    "\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            sort_key=lambda x: len(x.text),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set seed.\n",
    "torch.manual_seed(1997)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# instantiate the model\n",
    "# instantiate the model\n",
    "l4_MultiheadAttentionClassifier = MultiheadAttentionClassifier(\n",
    "    n_classes=1,\n",
    "    num_embeddings=len(TEXT.vocab),\n",
    "    embedding_dim=300,\n",
    "    num_layers=1,\n",
    "    heads=2,\n",
    "    device=\"cuda\",\n",
    "    augmentation_factor=4,\n",
    "    dropout=0,\n",
    "    max_length=22,\n",
    "    pad_idx=TEXT.vocab.stoi[TEXT.pad_token]).to(\"cuda\")\n",
    "\n",
    "print(l4_MultiheadAttentionClassifier, \"\\n\")\n",
    "print(f'The model has {count_parameters(l4_MultiheadAttentionClassifier):,} trainable parameters.', \"\\n\")\n",
    "\n",
    "# Let us use Adam.\n",
    "OPT = optim.Adam(l4_MultiheadAttentionClassifier.parameters(), lr=0.001)\n",
    "\n",
    "# Specify criterion.\n",
    "CRIT = nn.PoissonNLLLoss()\n",
    "\n",
    "MultiheadAttentionClassifier_train(l4_MultiheadAttentionClassifier,\n",
    "      train_iterator,\n",
    "      valid_iterator,\n",
    "      n_epochs,\n",
    "      OPT,\n",
    "      CRIT,\n",
    "      accuracy_function=poisson_accuracy,\n",
    "      save=True,\n",
    "      saving_path=\"model_bck/l4_MultiheadAttentionClassifier.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

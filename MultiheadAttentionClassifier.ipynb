{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd95730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.neural_net_tools import *\n",
    "# from utils.multihead_attention_classifier import *\n",
    "from utils.multihead_attention_classifier_v2 import *\n",
    "import torch\n",
    "from torchtext import data\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "path_to_data = \"dataset/text_molecule.csv\"\n",
    "format_ = \"csv\"\n",
    "split_ratio = 5/6\n",
    "min_freq_words = 1\n",
    "batch_size = 384\n",
    "seed = 1997\n",
    "n_epochs = 200\n",
    "fix_length = 22 #51 with H and 22 without.\n",
    "\n",
    "# check whether cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8e322",
   "metadata": {},
   "source": [
    "# Normal label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5265c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 938 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  include_lengths=True, fix_length=fix_length)\n",
    "\n",
    "LABEL = data.Field(sequential=False, dtype=torch.float, batch_first=True,\n",
    "                   use_vocab=False, preprocessing=float)\n",
    "\n",
    "fields = [('text', TEXT), ('label', LABEL),\n",
    "          (None, None), (None, None), (None, None)]\n",
    "\n",
    "training_data = data.TabularDataset(path=path_to_data,\n",
    "                                    format=format_,\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=split_ratio,\n",
    "                                             random_state=random.seed(seed))\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=min_freq_words)\n",
    "\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            sort_key=lambda x: len(x.text),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08194d3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiheadAttentionClassifier(\n",
      "  (encoder): Encoder(\n",
      "    (tok_embedding): Embedding(15, 256)\n",
      "    (pos_embedding): Embedding(22, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadScaledDotProductAttention(\n",
      "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (LayerNormalization): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (LayerNormalization1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (LayerNormalization2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (linear_augmentation): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadScaledDotProductAttention(\n",
      "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (LayerNormalization): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (LayerNormalization1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (LayerNormalization2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (linear_augmentation): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (linear3): Linear(in_features=256, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "The model has 1,721,879 trainable parameters. \n",
      "\n",
      "\tNo. epoch: 1/200                     \n",
      "\t   Train Loss: 0.26\n",
      "\t    Val. Loss: 0.153\n",
      "\n",
      "\n",
      "\tNo. epoch: 2/200                     \n",
      "\t   Train Loss: 0.136\n",
      "\t    Val. Loss: 0.126\n",
      "\n",
      "\n",
      "\tNo. epoch: 3/200                     \n",
      "\t   Train Loss: 0.111\n",
      "\t    Val. Loss: 0.107\n",
      "\n",
      "\n",
      "\tNo. epoch: 4/200                     \n",
      "\t   Train Loss: 0.092\n",
      "\t    Val. Loss: 0.082\n",
      "\n",
      "\n",
      "\tNo. epoch: 5/200                     \n",
      "\t   Train Loss: 0.077\n",
      "\t    Val. Loss: 0.073\n",
      "\n",
      "\n",
      "\tNo. epoch: 6/200                     \n",
      "\t   Train Loss: 0.076\n",
      "\t    Val. Loss: 0.069\n",
      "\n",
      "\n",
      "\tNo. epoch: 7/200                     \n",
      "\t   Train Loss: 0.056\n",
      "\t    Val. Loss: 0.059\n",
      "\n",
      "\n",
      "\tNo. epoch: 8/200                     \n",
      "\t   Train Loss: 0.05\n",
      "\t    Val. Loss: 0.057\n",
      "\n",
      "\n",
      "\tNo. epoch: 9/200                     \n",
      "\t   Train Loss: 0.048\n",
      "\t    Val. Loss: 0.048\n",
      "\n",
      "\n",
      "\tNo. epoch: 10/200                    \n",
      "\t   Train Loss: 0.036\n",
      "\t    Val. Loss: 0.041\n",
      "\n",
      "\n",
      "\tNo. epoch: 11/200                    \n",
      "\t   Train Loss: 0.037\n",
      "\t    Val. Loss: 0.039\n",
      "\n",
      "\n",
      "\tNo. epoch: 12/200                    \n",
      "\t   Train Loss: 0.031\n",
      "\t    Val. Loss: 0.036\n",
      "\n",
      "\n",
      "\tNo. epoch: 13/200                    \n",
      "\t   Train Loss: 0.03\n",
      "\t    Val. Loss: 0.031\n",
      "\n",
      "\n",
      "\tNo. epoch: 14/200                    \n",
      "\t   Train Loss: 0.026\n",
      "\t    Val. Loss: 0.031\n",
      "\n",
      "\n",
      "\tNo. epoch: 15/200                    \n",
      "\t   Train Loss: 0.028\n",
      "\t    Val. Loss: 0.027\n",
      "\n",
      "\n",
      "\tNo. epoch: 16/200                    \n",
      "\t   Train Loss: 0.025\n",
      "\t    Val. Loss: 0.025\n",
      "\n",
      "\n",
      "\tNo. epoch: 17/200                    \n",
      "\t   Train Loss: 0.029\n",
      "\t    Val. Loss: 0.044\n",
      "\n",
      "\n",
      "\tNo. epoch: 18/200                    \n",
      "\t   Train Loss: 0.021\n",
      "\t    Val. Loss: 0.027\n",
      "\n",
      "\n",
      "\tNo. epoch: 19/200                    \n",
      "\t   Train Loss: 0.023\n",
      "\t    Val. Loss: 0.025\n",
      "\n",
      "\n",
      "\tNo. epoch: 20/200                    \n",
      "\t   Train Loss: 0.015\n",
      "\t    Val. Loss: 0.024\n",
      "\n",
      "\n",
      "\tNo. epoch: 21/200                    \n",
      "\t   Train Loss: 0.014\n",
      "\t    Val. Loss: 0.022\n",
      "\n",
      "\n",
      "\tNo. epoch: 22/200                    \n",
      "\t   Train Loss: 0.016\n",
      "\t    Val. Loss: 0.033\n",
      "\n",
      "\n",
      "\tNo. epoch: 23/200                    \n",
      "\t   Train Loss: 0.015\n",
      "\t    Val. Loss: 0.023\n",
      "\n",
      "\n",
      "\tNo. epoch: 24/200                    \n",
      "\t   Train Loss: 0.019\n",
      "\t    Val. Loss: 0.021\n",
      "\n",
      "\n",
      "\tNo. epoch: 25/200                    \n",
      "\t   Train Loss: 0.013\n",
      "\t    Val. Loss: 0.022\n",
      "\n",
      "\n",
      "\tNo. epoch: 26/200                    \n",
      "\t   Train Loss: 0.011\n",
      "\t    Val. Loss: 0.023\n",
      "\n",
      "\n",
      "\tNo. epoch: 27/200                    \n",
      "\t   Train Loss: 0.018\n",
      "\t    Val. Loss: 0.03\n",
      "\n",
      "\n",
      "\tNo. epoch: 28/200                    \n",
      "\t   Train Loss: 0.016\n",
      "\t    Val. Loss: 0.018\n",
      "\n",
      "\n",
      "\tNo. epoch: 29/200                    \n",
      "\t   Train Loss: 0.013\n",
      "\t    Val. Loss: 0.017\n",
      "\n",
      "\n",
      "\tNo. epoch: 30/200                    \n",
      "\t   Train Loss: 0.009\n",
      "\t    Val. Loss: 0.016\n",
      "\n",
      "\n",
      "\tNo. epoch: 31/200                    \n",
      "\t   Train Loss: 0.009\n",
      "\t    Val. Loss: 0.019\n",
      "\n",
      "\n",
      "\tNo. epoch: 32/200                    \n",
      "\t   Train Loss: 0.011\n",
      "\t    Val. Loss: 0.017\n",
      "\n",
      "\n",
      "\tNo. epoch: 33/200                    \n",
      "\t   Train Loss: 0.018\n",
      "\t    Val. Loss: 0.096\n",
      "\n",
      "\n",
      "\tNo. epoch: 34/200                    \n",
      "\t   Train Loss: 0.018\n",
      "\t    Val. Loss: 0.022\n",
      "\n",
      "\n",
      "\tNo. epoch: 35/200                    \n",
      "\t   Train Loss: 0.007\n",
      "\t    Val. Loss: 0.015\n",
      "\n",
      "\n",
      "\tNo. epoch: 36/200                    \n",
      "\t   Train Loss: 0.008\n",
      "\t    Val. Loss: 0.017\n",
      "\n",
      "\n",
      "\tNo. epoch: 37/200                    \n",
      "\t   Train Loss: 0.007\n",
      "\t    Val. Loss: 0.014\n",
      "\n",
      "\n",
      "\tNo. epoch: 38/200                    \n",
      "\t   Train Loss: 0.009\n",
      "\t    Val. Loss: 0.013\n",
      "\n",
      "\n",
      "\tNo. epoch: 39/200                    \n",
      "\t   Train Loss: 0.006\n",
      "\t    Val. Loss: 0.013\n",
      "\n",
      "\n",
      "\tNo. epoch: 40/200                    \n",
      "\t   Train Loss: 0.005\n",
      "\t    Val. Loss: 0.013\n",
      "\n",
      "\n",
      "\tNo. epoch: 41/200                    \n",
      "\t   Train Loss: 0.024\n",
      "\t    Val. Loss: 0.017\n",
      "\n",
      "\n",
      "\tNo. epoch: 42/200                    \n",
      "\t   Train Loss: 0.007\n",
      "\t    Val. Loss: 0.013\n",
      "\n",
      "\n",
      "\tNo. epoch: 43/200                    \n",
      "\t   Train Loss: 0.007\n",
      "\t    Val. Loss: 0.014\n",
      "\n",
      "\n",
      "\tNo. epoch: 44/200                    \n",
      "\t   Train Loss: 0.007\n",
      "\t    Val. Loss: 0.015\n",
      "\n",
      "\n",
      "\tNo. epoch: 45/200                    \n",
      "\t   Train Loss: 0.006\n",
      "\t    Val. Loss: 0.012\n",
      "\n",
      "\n",
      "\tNo. epoch: 46/200                    \n",
      "\t   Train Loss: 0.006\n",
      "\t    Val. Loss: 0.019\n",
      "\n",
      "\n",
      "\tNo. epoch: 47/200                    \n",
      "\t   Train Loss: 0.009\n",
      "\t    Val. Loss: 0.015\n",
      "\n",
      "\n",
      "\tNo. epoch: 48/200                    \n",
      "\t   Train Loss: 0.006\n",
      "\t    Val. Loss: 0.013\n",
      "\n",
      "\n",
      "\tNo. epoch: 49/200                    \n",
      "\t   Train Loss: 0.004\n",
      "\t    Val. Loss: 0.012\n",
      "\n",
      "\n",
      "\tNo. epoch: 50/200                    \n",
      "\t   Train Loss: 0.012\n",
      "\t    Val. Loss: 0.013\n",
      "\n",
      "\n",
      "\tNo. epoch: 51/200                    \n",
      "\t   Train Loss: 0.008\n",
      "\t    Val. Loss: 0.017\n",
      "\n",
      "\n",
      "\tNo. epoch: 52/200                    \n",
      "\t   Train Loss: 0.007\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 53/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 54/200                    \n",
      "\t   Train Loss: 0.004\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 55/200                    \n",
      "\t   Train Loss: 0.004\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 56/200                    \n",
      "\t   Train Loss: 0.014\n",
      "\t    Val. Loss: 0.014\n",
      "\n",
      "\n",
      "\tNo. epoch: 57/200                    \n",
      "\t   Train Loss: 0.005\n",
      "\t    Val. Loss: 0.015\n",
      "\n",
      "\n",
      "\tNo. epoch: 58/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 59/200                    \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 60/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 61/200                    \n",
      "\t   Train Loss: 0.02\n",
      "\t    Val. Loss: 0.066\n",
      "\n",
      "\n",
      "\tNo. epoch: 62/200                    \n",
      "\t   Train Loss: 0.015\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 63/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 64/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 65/200                    \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 66/200                    \n",
      "\t   Train Loss: 0.004\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 67/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 68/200                    \n",
      "\t   Train Loss: 0.005\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 69/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 70/200                    \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNo. epoch: 71/200                    \n",
      "\t   Train Loss: 0.005\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 72/200                    \n",
      "\t   Train Loss: 0.006\n",
      "\t    Val. Loss: 0.013\n",
      "\n",
      "\n",
      "\tNo. epoch: 73/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 74/200                    \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 75/200                    \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 76/200                    \n",
      "\t   Train Loss: 0.016\n",
      "\t    Val. Loss: 0.015\n",
      "\n",
      "\n",
      "\tNo. epoch: 77/200                    \n",
      "\t   Train Loss: 0.006\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 78/200                    \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 79/200                    \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 80/200                    \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 81/200                    \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 82/200                    \n",
      "\t   Train Loss: 0.004\n",
      "\t    Val. Loss: 0.012\n",
      "\n",
      "\n",
      "\tNo. epoch: 83/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 84/200                    \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 85/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 86/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.013\n",
      "\n",
      "\n",
      "\tNo. epoch: 87/200                    \n",
      "\t   Train Loss: 0.005\n",
      "\t    Val. Loss: 0.013\n",
      "\n",
      "\n",
      "\tNo. epoch: 88/200                    \n",
      "\t   Train Loss: 0.01\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 89/200                    \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 90/200                    \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 91/200                    \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 92/200                    \n",
      "\t   Train Loss: 0.004\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 93/200                    \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 94/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 95/200                    \n",
      "\t   Train Loss: 0.004\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 96/200                    \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 97/200                    \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 98/200                    \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.015\n",
      "\n",
      "\n",
      "\tNo. epoch: 99/200                    \n",
      "\t   Train Loss: 0.01\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 100/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 101/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 102/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 103/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 104/200                   \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 105/200                   \n",
      "\t   Train Loss: 0.008\n",
      "\t    Val. Loss: 0.012\n",
      "\n",
      "\n",
      "\tNo. epoch: 106/200                   \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 107/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 108/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 109/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 110/200                   \n",
      "\t   Train Loss: 0.009\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 111/200                   \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 112/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 113/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 114/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 115/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 116/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.014\n",
      "\n",
      "\n",
      "\tNo. epoch: 117/200                   \n",
      "\t   Train Loss: 0.004\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 118/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 119/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 120/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 121/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 122/200                   \n",
      "\t   Train Loss: 0.005\n",
      "\t    Val. Loss: 0.012\n",
      "\n",
      "\n",
      "\tNo. epoch: 123/200                   \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 124/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 125/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 126/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 127/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 128/200                   \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.015\n",
      "\n",
      "\n",
      "\tNo. epoch: 129/200                   \n",
      "\t   Train Loss: 0.008\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 130/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 131/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 132/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 133/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 134/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 135/200                   \n",
      "\t   Train Loss: 0.005\n",
      "\t    Val. Loss: 0.012\n",
      "\n",
      "\n",
      "\tNo. epoch: 136/200                   \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 137/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 138/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 139/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 140/200                   \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 141/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 142/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 143/200                   \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 144/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 145/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 146/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 147/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.012\n",
      "\n",
      "\n",
      "\tNo. epoch: 148/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 149/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 150/200                   \n",
      "\t   Train Loss: 0.005\n",
      "\t    Val. Loss: 0.012\n",
      "\n",
      "\n",
      "\tNo. epoch: 151/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 152/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 153/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 154/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 155/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.012\n",
      "\n",
      "\n",
      "\tNo. epoch: 156/200                   \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 157/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 158/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 159/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 160/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.01\n",
      "\n",
      "\n",
      "\tNo. epoch: 161/200                   \n",
      "\t   Train Loss: 0.004\n",
      "\t    Val. Loss: 0.012\n",
      "\n",
      "\n",
      "\tNo. epoch: 162/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 163/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 164/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 165/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 166/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 167/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNo. epoch: 168/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 169/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 170/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 171/200                   \n",
      "\t   Train Loss: 0.003\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 172/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 173/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 174/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 175/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 176/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 177/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 178/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 179/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 180/200                   \n",
      "\t   Train Loss: 0.005\n",
      "\t    Val. Loss: 0.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 181/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 182/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 183/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 184/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 185/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.012\n",
      "\n",
      "\n",
      "\tNo. epoch: 186/200                   \n",
      "\t   Train Loss: 0.005\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 187/200                   \n",
      "\t   Train Loss: 0.001\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 188/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 189/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 190/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 191/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 192/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.011\n",
      "\n",
      "\n",
      "\tNo. epoch: 193/200                   \n",
      "\t   Train Loss: 0.004\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "\tNo. epoch: 194/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.016\n",
      "\n",
      "\n",
      "\tNo. epoch: 195/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 196/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 197/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 198/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 199/200                   \n",
      "\t   Train Loss: 0.0\n",
      "\t    Val. Loss: 0.007\n",
      "\n",
      "\n",
      "\tNo. epoch: 200/200                   \n",
      "\t   Train Loss: 0.002\n",
      "\t    Val. Loss: 0.008\n",
      "\n",
      "\n",
      "Wall time: 40min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set seed.\n",
    "torch.manual_seed(1997)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# instantiate the model\n",
    "l1_MultiheadAttentionClassifier = MultiheadAttentionClassifier(\n",
    "    n_classes=1,\n",
    "    num_embeddings=len(TEXT.vocab),\n",
    "    embedding_dim=256,\n",
    "    num_layers=2,\n",
    "    heads=8,\n",
    "    device=\"cuda\",\n",
    "    augmentation_factor=4,\n",
    "    dropout=0,\n",
    "    max_length=fix_length,\n",
    "    pad_idx=TEXT.vocab.stoi[TEXT.pad_token]).to(\"cuda\")\n",
    "\n",
    "print(l1_MultiheadAttentionClassifier, \"\\n\")\n",
    "print(f'The model has {count_parameters(l1_MultiheadAttentionClassifier):,} trainable parameters.', \"\\n\")\n",
    "\n",
    "# Let us use Adam.\n",
    "OPT = optim.Adam(l1_MultiheadAttentionClassifier.parameters(), lr=0.0001)\n",
    "\n",
    "# Specify criterion.\n",
    "CRIT = nn.MSELoss()\n",
    "\n",
    "MultiheadAttentionClassifier_train(l1_MultiheadAttentionClassifier,\n",
    "                                   train_iterator,\n",
    "                                   valid_iterator,\n",
    "                                   n_epochs,\n",
    "                                   OPT,\n",
    "                                   CRIT,\n",
    "                                   accuracy_function=None,\n",
    "                                   save=True,\n",
    "                                   saving_path=\"model_bck/l1_MultiheadAttentionClassifier_lr0.0001_B384_D0_GC_false.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ee556",
   "metadata": {},
   "source": [
    "# Poisson label 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477d269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  include_lengths=True, fix_length=fix_length)\n",
    "\n",
    "LABEL = data.Field(sequential=False, dtype=torch.float, batch_first=True,\n",
    "                   use_vocab=False, preprocessing=float)\n",
    "\n",
    "fields = [('text', TEXT), (None, None),\n",
    "          ('label', LABEL), (None, None), (None, None)]\n",
    "\n",
    "training_data = data.TabularDataset(path=path_to_data,\n",
    "                                    format=format_,\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=split_ratio,\n",
    "                                             random_state=random.seed(seed))\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=min_freq_words)\n",
    "\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            sort_key=lambda x: len(x.text),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ee5428",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiheadAttentionClassifier(\n",
      "  (encoder): Encoder(\n",
      "    (tok_embedding): Embedding(15, 256)\n",
      "    (pos_embedding): Embedding(22, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadScaledDotProductAttention(\n",
      "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (LayerNormalization): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (LayerNormalization1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (LayerNormalization2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (linear_augmentation): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadScaledDotProductAttention(\n",
      "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (LayerNormalization): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (LayerNormalization1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (LayerNormalization2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (linear_augmentation): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (linear3): Linear(in_features=256, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "The model has 1,721,879 trainable parameters. \n",
      "\n",
      "\tNo. epoch: 1/200                     \n",
      "\t   Train Loss: 0.666 | Train Acc: 61.396%\n",
      "\t    Val. Loss: 0.562 |  Val. Acc: 72.033%\n",
      "\n",
      "\n",
      "\tNo. epoch: 2/200                     \n",
      "\t   Train Loss: 0.496 | Train Acc: 75.991%\n",
      "\t    Val. Loss: 0.451 |  Val. Acc: 80.243%\n",
      "\n",
      "\n",
      "\tNo. epoch: 3/200                     \n",
      "\t   Train Loss: 0.43 | Train Acc: 81.533%\n",
      "\t    Val. Loss: 0.421 |  Val. Acc: 83.184%\n",
      "\n",
      "\n",
      "\tNo. epoch: 4/200                     \n",
      "\t   Train Loss: 0.393 | Train Acc: 84.977%\n",
      "\t    Val. Loss: 0.379 |  Val. Acc: 86.334%\n",
      "\n",
      "\n",
      "\tNo. epoch: 5/200                     \n",
      "\t   Train Loss: 0.361 | Train Acc: 88.289%\n",
      "\t    Val. Loss: 0.343 |  Val. Acc: 89.939%\n",
      "\n",
      "\n",
      "\tNo. epoch: 6/200                     \n",
      "\t   Train Loss: 0.339 | Train Acc: 90.919%\n",
      "\t    Val. Loss: 0.327 |  Val. Acc: 92.219%\n",
      "\n",
      "\n",
      "\tNo. epoch: 7/200                     \n",
      "\t   Train Loss: 0.324 | Train Acc: 92.623%\n",
      "\t    Val. Loss: 0.326 |  Val. Acc: 92.956%\n",
      "\n",
      "\n",
      "\tNo. epoch: 8/200                     \n",
      "\t   Train Loss: 0.32 | Train Acc: 92.726%\n",
      "\t    Val. Loss: 0.307 |  Val. Acc: 95.306%\n",
      "\n",
      "\n",
      "\tNo. epoch: 9/200                     \n",
      "\t   Train Loss: 0.303 | Train Acc: 95.587%\n",
      "\t    Val. Loss: 0.307 |  Val. Acc: 94.658%\n",
      "\n",
      "\n",
      "\tNo. epoch: 10/200                    \n",
      "\t   Train Loss: 0.302 | Train Acc: 95.629%\n",
      "\t    Val. Loss: 0.304 |  Val. Acc: 94.963%\n",
      "\n",
      "\n",
      "\tNo. epoch: 11/200                    \n",
      "\t   Train Loss: 0.297 | Train Acc: 96.412%\n",
      "\t    Val. Loss: 0.298 |  Val. Acc: 96.449%\n",
      "\n",
      "\n",
      "\tNo. epoch: 12/200                    \n",
      "\t   Train Loss: 0.292 | Train Acc: 97.656%\n",
      "\t    Val. Loss: 0.296 |  Val. Acc: 97.091%\n",
      "\n",
      "\n",
      "\tNo. epoch: 13/200                    \n",
      "\t   Train Loss: 0.292 | Train Acc: 97.769%\n",
      "\t    Val. Loss: 0.295 |  Val. Acc: 97.377%\n",
      "\n",
      "\n",
      "\tNo. epoch: 14/200                    \n",
      "\t   Train Loss: 0.29 | Train Acc: 98.075%\n",
      "\t    Val. Loss: 0.298 |  Val. Acc: 96.792%\n",
      "\n",
      "\n",
      "\tNo. epoch: 15/200                    \n",
      "\t   Train Loss: 0.322 | Train Acc: 92.655%\n",
      "\t    Val. Loss: 0.312 |  Val. Acc: 92.368%\n",
      "\n",
      "\n",
      "\tNo. epoch: 16/200                    \n",
      "\t   Train Loss: 0.294 | Train Acc: 97.178%\n",
      "\t    Val. Loss: 0.296 |  Val. Acc: 96.754%\n",
      "\n",
      "\n",
      "\tNo. epoch: 17/200                    \n",
      "\t   Train Loss: 0.29 | Train Acc: 98.021%\n",
      "\t    Val. Loss: 0.295 |  Val. Acc: 96.843%\n",
      "\n",
      "\n",
      "\tNo. epoch: 18/200                    \n",
      "\t   Train Loss: 0.289 | Train Acc: 98.298%\n",
      "\t    Val. Loss: 0.294 |  Val. Acc: 96.792%\n",
      "\n",
      "\n",
      "\tNo. epoch: 19/200                    \n",
      "\t   Train Loss: 0.288 | Train Acc: 98.577%\n",
      "\t    Val. Loss: 0.292 |  Val. Acc: 98.05%\n",
      "\n",
      "\n",
      "\tNo. epoch: 20/200                    \n",
      "\t   Train Loss: 0.288 | Train Acc: 98.643%\n",
      "\t    Val. Loss: 0.301 |  Val. Acc: 96.551%\n",
      "\n",
      "\n",
      "\tNo. epoch: 21/200                    \n",
      "\t   Train Loss: 0.29 | Train Acc: 98.197%\n",
      "\t    Val. Loss: 0.29 |  Val. Acc: 98.38%\n",
      "\n",
      "\n",
      "\tNo. epoch: 22/200                    \n",
      "\t   Train Loss: 0.288 | Train Acc: 98.613%\n",
      "\t    Val. Loss: 0.292 |  Val. Acc: 98.133%\n",
      "\n",
      "\n",
      "\tNo. epoch: 23/200                    \n",
      "\t   Train Loss: 0.286 | Train Acc: 99.157%\n",
      "\t    Val. Loss: 0.291 |  Val. Acc: 98.202%\n",
      "\n",
      "\n",
      "\tNo. epoch: 24/200                    \n",
      "\t   Train Loss: 0.286 | Train Acc: 99.12%\n",
      "\t    Val. Loss: 0.291 |  Val. Acc: 97.904%\n",
      "\n",
      "\n",
      "\tNo. epoch: 25/200                    \n",
      "\t   Train Loss: 0.311 | Train Acc: 94.174%\n",
      "\t    Val. Loss: 0.301 |  Val. Acc: 96.138%\n",
      "\n",
      "\n",
      "\tNo. epoch: 26/200                    \n",
      "\t   Train Loss: 0.293 | Train Acc: 97.204%\n",
      "\t    Val. Loss: 0.296 |  Val. Acc: 95.846%\n",
      "\n",
      "\n",
      "\tNo. epoch: 27/200                    \n",
      "\t   Train Loss: 0.287 | Train Acc: 98.971%\n",
      "\t    Val. Loss: 0.289 |  Val. Acc: 98.577%\n",
      "\n",
      "\n",
      "\tNo. epoch: 28/200                    \n",
      "\t   Train Loss: 0.285 | Train Acc: 99.298%\n",
      "\t    Val. Loss: 0.289 |  Val. Acc: 98.831%\n",
      "\n",
      "\n",
      "\tNo. epoch: 29/200                    \n",
      "\t   Train Loss: 0.285 | Train Acc: 99.421%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 98.8%\n",
      "\n",
      "\n",
      "\tNo. epoch: 30/200                    \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.602%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 99.047%\n",
      "\n",
      "\n",
      "\tNo. epoch: 31/200                    \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.505%\n",
      "\t    Val. Loss: 0.29 |  Val. Acc: 98.717%\n",
      "\n",
      "\n",
      "\tNo. epoch: 32/200                    \n",
      "\t   Train Loss: 0.295 | Train Acc: 97.176%\n",
      "\t    Val. Loss: 0.319 |  Val. Acc: 92.349%\n",
      "\n",
      "\n",
      "\tNo. epoch: 33/200                    \n",
      "\t   Train Loss: 0.308 | Train Acc: 94.081%\n",
      "\t    Val. Loss: 0.304 |  Val. Acc: 94.703%\n",
      "\n",
      "\n",
      "\tNo. epoch: 34/200                    \n",
      "\t   Train Loss: 0.291 | Train Acc: 97.54%\n",
      "\t    Val. Loss: 0.291 |  Val. Acc: 98.145%\n",
      "\n",
      "\n",
      "\tNo. epoch: 35/200                    \n",
      "\t   Train Loss: 0.285 | Train Acc: 99.263%\n",
      "\t    Val. Loss: 0.289 |  Val. Acc: 98.571%\n",
      "\n",
      "\n",
      "\tNo. epoch: 36/200                    \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.543%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 99.003%\n",
      "\n",
      "\n",
      "\tNo. epoch: 37/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.7%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 99.06%\n",
      "\n",
      "\n",
      "\tNo. epoch: 38/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.784%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.143%\n",
      "\n",
      "\n",
      "\tNo. epoch: 39/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.748%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 98.939%\n",
      "\n",
      "\n",
      "\tNo. epoch: 40/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.751%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 99.117%\n",
      "\n",
      "\n",
      "\tNo. epoch: 41/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.775%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 99.092%\n",
      "\n",
      "\n",
      "\tNo. epoch: 42/200                    \n",
      "\t   Train Loss: 0.289 | Train Acc: 98.027%\n",
      "\t    Val. Loss: 0.299 |  Val. Acc: 96.221%\n",
      "\n",
      "\n",
      "\tNo. epoch: 43/200                    \n",
      "\t   Train Loss: 0.294 | Train Acc: 97.157%\n",
      "\t    Val. Loss: 0.299 |  Val. Acc: 95.865%\n",
      "\n",
      "\n",
      "\tNo. epoch: 44/200                    \n",
      "\t   Train Loss: 0.287 | Train Acc: 98.791%\n",
      "\t    Val. Loss: 0.289 |  Val. Acc: 98.755%\n",
      "\n",
      "\n",
      "\tNo. epoch: 45/200                    \n",
      "\t   Train Loss: 0.286 | Train Acc: 99.187%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 98.831%\n",
      "\n",
      "\n",
      "\tNo. epoch: 46/200                    \n",
      "\t   Train Loss: 0.285 | Train Acc: 99.031%\n",
      "\t    Val. Loss: 0.291 |  Val. Acc: 98.006%\n",
      "\n",
      "\n",
      "\tNo. epoch: 47/200                    \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.467%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 98.946%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNo. epoch: 48/200                    \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.656%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 98.711%\n",
      "\n",
      "\n",
      "\tNo. epoch: 49/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.724%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.162%\n",
      "\n",
      "\n",
      "\tNo. epoch: 50/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.843%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.409%\n",
      "\n",
      "\n",
      "\tNo. epoch: 51/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.791%\n",
      "\t    Val. Loss: 0.289 |  Val. Acc: 99.219%\n",
      "\n",
      "\n",
      "\tNo. epoch: 52/200                    \n",
      "\t   Train Loss: 0.286 | Train Acc: 99.149%\n",
      "\t    Val. Loss: 0.31 |  Val. Acc: 95.433%\n",
      "\n",
      "\n",
      "\tNo. epoch: 53/200                    \n",
      "\t   Train Loss: 0.309 | Train Acc: 95.084%\n",
      "\t    Val. Loss: 0.291 |  Val. Acc: 98.234%\n",
      "\n",
      "\n",
      "\tNo. epoch: 54/200                    \n",
      "\t   Train Loss: 0.285 | Train Acc: 99.396%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 98.787%\n",
      "\n",
      "\n",
      "\tNo. epoch: 55/200                    \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.661%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.32%\n",
      "\n",
      "\n",
      "\tNo. epoch: 56/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.837%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.308%\n",
      "\n",
      "\n",
      "\tNo. epoch: 57/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.891%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.352%\n",
      "\n",
      "\n",
      "\tNo. epoch: 58/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.904%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.441%\n",
      "\n",
      "\n",
      "\tNo. epoch: 59/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.903%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.409%\n",
      "\n",
      "\n",
      "\tNo. epoch: 60/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.929%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.333%\n",
      "\n",
      "\n",
      "\tNo. epoch: 61/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.926%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.314%\n",
      "\n",
      "\n",
      "\tNo. epoch: 62/200                    \n",
      "\t   Train Loss: 0.294 | Train Acc: 97.048%\n",
      "\t    Val. Loss: 0.305 |  Val. Acc: 95.328%\n",
      "\n",
      "\n",
      "\tNo. epoch: 63/200                    \n",
      "\t   Train Loss: 0.299 | Train Acc: 95.899%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 98.609%\n",
      "\n",
      "\n",
      "\tNo. epoch: 64/200                    \n",
      "\t   Train Loss: 0.285 | Train Acc: 99.346%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 98.958%\n",
      "\n",
      "\n",
      "\tNo. epoch: 65/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.753%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.295%\n",
      "\n",
      "\n",
      "\tNo. epoch: 66/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.891%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.428%\n",
      "\n",
      "\n",
      "\tNo. epoch: 67/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.914%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.454%\n",
      "\n",
      "\n",
      "\tNo. epoch: 68/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.937%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.479%\n",
      "\n",
      "\n",
      "\tNo. epoch: 69/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.951%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.524%\n",
      "\n",
      "\n",
      "\tNo. epoch: 70/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.947%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.562%\n",
      "\n",
      "\n",
      "\tNo. epoch: 71/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.953%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.555%\n",
      "\n",
      "\n",
      "\tNo. epoch: 72/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.658%\n",
      "\t    Val. Loss: 0.289 |  Val. Acc: 98.406%\n",
      "\n",
      "\n",
      "\tNo. epoch: 73/200                    \n",
      "\t   Train Loss: 0.288 | Train Acc: 98.585%\n",
      "\t    Val. Loss: 0.316 |  Val. Acc: 91.908%\n",
      "\n",
      "\n",
      "\tNo. epoch: 74/200                    \n",
      "\t   Train Loss: 0.303 | Train Acc: 95.051%\n",
      "\t    Val. Loss: 0.289 |  Val. Acc: 97.98%\n",
      "\n",
      "\n",
      "\tNo. epoch: 75/200                    \n",
      "\t   Train Loss: 0.285 | Train Acc: 99.399%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.143%\n",
      "\n",
      "\n",
      "\tNo. epoch: 76/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.76%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 98.946%\n",
      "\n",
      "\n",
      "\tNo. epoch: 77/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.799%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.447%\n",
      "\n",
      "\n",
      "\tNo. epoch: 78/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.936%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.422%\n",
      "\n",
      "\n",
      "\tNo. epoch: 79/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.697%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.486%\n",
      "\n",
      "\n",
      "\tNo. epoch: 80/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.931%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.517%\n",
      "\n",
      "\n",
      "\tNo. epoch: 81/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.952%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.505%\n",
      "\n",
      "\n",
      "\tNo. epoch: 82/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.956%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.587%\n",
      "\n",
      "\n",
      "\tNo. epoch: 83/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.96%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.536%\n",
      "\n",
      "\n",
      "\tNo. epoch: 84/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.966%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.555%\n",
      "\n",
      "\n",
      "\tNo. epoch: 85/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.971%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.593%\n",
      "\n",
      "\n",
      "\tNo. epoch: 86/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.958%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.39%\n",
      "\n",
      "\n",
      "\tNo. epoch: 87/200                    \n",
      "\t   Train Loss: 0.293 | Train Acc: 97.372%\n",
      "\t    Val. Loss: 0.291 |  Val. Acc: 97.704%\n",
      "\n",
      "\n",
      "\tNo. epoch: 88/200                    \n",
      "\t   Train Loss: 0.288 | Train Acc: 98.547%\n",
      "\t    Val. Loss: 0.291 |  Val. Acc: 97.91%\n",
      "\n",
      "\n",
      "\tNo. epoch: 89/200                    \n",
      "\t   Train Loss: 0.286 | Train Acc: 98.746%\n",
      "\t    Val. Loss: 0.289 |  Val. Acc: 99.098%\n",
      "\n",
      "\n",
      "\tNo. epoch: 90/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.719%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.181%\n",
      "\n",
      "\n",
      "\tNo. epoch: 91/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.818%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.251%\n",
      "\n",
      "\n",
      "\tNo. epoch: 92/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.724%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.403%\n",
      "\n",
      "\n",
      "\tNo. epoch: 93/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.789%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.384%\n",
      "\n",
      "\n",
      "\tNo. epoch: 94/200                    \n",
      "\t   Train Loss: 0.288 | Train Acc: 98.429%\n",
      "\t    Val. Loss: 0.293 |  Val. Acc: 98.006%\n",
      "\n",
      "\n",
      "\tNo. epoch: 95/200                    \n",
      "\t   Train Loss: 0.286 | Train Acc: 98.966%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.123%\n",
      "\n",
      "\n",
      "\tNo. epoch: 96/200                    \n",
      "\t   Train Loss: 0.285 | Train Acc: 99.413%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 99.149%\n",
      "\n",
      "\n",
      "\tNo. epoch: 97/200                    \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.848%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.454%\n",
      "\n",
      "\n",
      "\tNo. epoch: 98/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.933%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.549%\n",
      "\n",
      "\n",
      "\tNo. epoch: 99/200                    \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.951%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.555%\n",
      "\n",
      "\n",
      "\tNo. epoch: 100/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.96%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.613%\n",
      "\n",
      "\n",
      "\tNo. epoch: 101/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.966%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.555%\n",
      "\n",
      "\n",
      "\tNo. epoch: 102/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.965%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.619%\n",
      "\n",
      "\n",
      "\tNo. epoch: 103/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.969%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.613%\n",
      "\n",
      "\n",
      "\tNo. epoch: 104/200                   \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.801%\n",
      "\t    Val. Loss: 0.29 |  Val. Acc: 98.336%\n",
      "\n",
      "\n",
      "\tNo. epoch: 105/200                   \n",
      "\t   Train Loss: 0.291 | Train Acc: 97.595%\n",
      "\t    Val. Loss: 0.295 |  Val. Acc: 96.195%\n",
      "\n",
      "\n",
      "\tNo. epoch: 106/200                   \n",
      "\t   Train Loss: 0.286 | Train Acc: 98.78%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 98.996%\n",
      "\n",
      "\n",
      "\tNo. epoch: 107/200                   \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.799%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.517%\n",
      "\n",
      "\n",
      "\tNo. epoch: 108/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.953%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.447%\n",
      "\n",
      "\n",
      "\tNo. epoch: 109/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.959%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.562%\n",
      "\n",
      "\n",
      "\tNo. epoch: 110/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.977%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.568%\n",
      "\n",
      "\n",
      "\tNo. epoch: 111/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.982%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.651%\n",
      "\n",
      "\n",
      "\tNo. epoch: 112/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.986%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.593%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNo. epoch: 113/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.984%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.657%\n",
      "\n",
      "\n",
      "\tNo. epoch: 114/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.949%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.581%\n",
      "\n",
      "\n",
      "\tNo. epoch: 115/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.935%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.549%\n",
      "\n",
      "\n",
      "\tNo. epoch: 116/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.982%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.593%\n",
      "\n",
      "\n",
      "\tNo. epoch: 117/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.992%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.632%\n",
      "\n",
      "\n",
      "\tNo. epoch: 118/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.99%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.632%\n",
      "\n",
      "\n",
      "\tNo. epoch: 119/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.977%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.282%\n",
      "\n",
      "\n",
      "\tNo. epoch: 120/200                   \n",
      "\t   Train Loss: 0.306 | Train Acc: 95.615%\n",
      "\t    Val. Loss: 0.289 |  Val. Acc: 98.603%\n",
      "\n",
      "\n",
      "\tNo. epoch: 121/200                   \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.517%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.339%\n",
      "\n",
      "\n",
      "\tNo. epoch: 122/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.887%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.39%\n",
      "\n",
      "\n",
      "\tNo. epoch: 123/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.964%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.536%\n",
      "\n",
      "\n",
      "\tNo. epoch: 124/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.982%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.524%\n",
      "\n",
      "\n",
      "\tNo. epoch: 125/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.99%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.549%\n",
      "\n",
      "\n",
      "\tNo. epoch: 126/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.988%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.593%\n",
      "\n",
      "\n",
      "\tNo. epoch: 127/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.994%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.543%\n",
      "\n",
      "\n",
      "\tNo. epoch: 128/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.568%\n",
      "\n",
      "\n",
      "\tNo. epoch: 129/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.574%\n",
      "\n",
      "\n",
      "\tNo. epoch: 130/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.581%\n",
      "\n",
      "\n",
      "\tNo. epoch: 131/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.995%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.543%\n",
      "\n",
      "\n",
      "\tNo. epoch: 132/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.574%\n",
      "\n",
      "\n",
      "\tNo. epoch: 133/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.574%\n",
      "\n",
      "\n",
      "\tNo. epoch: 134/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.562%\n",
      "\n",
      "\n",
      "\tNo. epoch: 135/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.973%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.06%\n",
      "\n",
      "\n",
      "\tNo. epoch: 136/200                   \n",
      "\t   Train Loss: 0.294 | Train Acc: 97.074%\n",
      "\t    Val. Loss: 0.293 |  Val. Acc: 97.085%\n",
      "\n",
      "\n",
      "\tNo. epoch: 137/200                   \n",
      "\t   Train Loss: 0.286 | Train Acc: 99.062%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.047%\n",
      "\n",
      "\n",
      "\tNo. epoch: 138/200                   \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.778%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.517%\n",
      "\n",
      "\n",
      "\tNo. epoch: 139/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.947%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.574%\n",
      "\n",
      "\n",
      "\tNo. epoch: 140/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.975%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.663%\n",
      "\n",
      "\n",
      "\tNo. epoch: 141/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.981%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.651%\n",
      "\n",
      "\n",
      "\tNo. epoch: 142/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.676%\n",
      "\n",
      "\n",
      "\tNo. epoch: 143/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.682%\n",
      "\n",
      "\n",
      "\tNo. epoch: 144/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.651%\n",
      "\n",
      "\n",
      "\tNo. epoch: 145/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.663%\n",
      "\n",
      "\n",
      "\tNo. epoch: 146/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.644%\n",
      "\n",
      "\n",
      "\tNo. epoch: 147/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.638%\n",
      "\n",
      "\n",
      "\tNo. epoch: 148/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.638%\n",
      "\n",
      "\n",
      "\tNo. epoch: 149/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.651%\n",
      "\n",
      "\n",
      "\tNo. epoch: 150/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.663%\n",
      "\n",
      "\n",
      "\tNo. epoch: 151/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.994%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.632%\n",
      "\n",
      "\n",
      "\tNo. epoch: 152/200                   \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.578%\n",
      "\t    Val. Loss: 0.301 |  Val. Acc: 95.103%\n",
      "\n",
      "\n",
      "\tNo. epoch: 153/200                   \n",
      "\t   Train Loss: 0.295 | Train Acc: 96.963%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 99.009%\n",
      "\n",
      "\n",
      "\tNo. epoch: 154/200                   \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.534%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.46%\n",
      "\n",
      "\n",
      "\tNo. epoch: 155/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.955%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.498%\n",
      "\n",
      "\n",
      "\tNo. epoch: 156/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.977%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.67%\n",
      "\n",
      "\n",
      "\tNo. epoch: 157/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.99%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.67%\n",
      "\n",
      "\n",
      "\tNo. epoch: 158/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.994%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.663%\n",
      "\n",
      "\n",
      "\tNo. epoch: 159/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.994%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.689%\n",
      "\n",
      "\n",
      "\tNo. epoch: 160/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.995%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.689%\n",
      "\n",
      "\n",
      "\tNo. epoch: 161/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.994%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.676%\n",
      "\n",
      "\n",
      "\tNo. epoch: 162/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.651%\n",
      "\n",
      "\n",
      "\tNo. epoch: 163/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.701%\n",
      "\n",
      "\n",
      "\tNo. epoch: 164/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.873%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.219%\n",
      "\n",
      "\n",
      "\tNo. epoch: 165/200                   \n",
      "\t   Train Loss: 0.288 | Train Acc: 98.638%\n",
      "\t    Val. Loss: 0.309 |  Val. Acc: 91.53%\n",
      "\n",
      "\n",
      "\tNo. epoch: 166/200                   \n",
      "\t   Train Loss: 0.288 | Train Acc: 98.333%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.162%\n",
      "\n",
      "\n",
      "\tNo. epoch: 167/200                   \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.791%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.606%\n",
      "\n",
      "\n",
      "\tNo. epoch: 168/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.984%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.625%\n",
      "\n",
      "\n",
      "\tNo. epoch: 169/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.992%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.701%\n",
      "\n",
      "\n",
      "\tNo. epoch: 170/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.721%\n",
      "\n",
      "\n",
      "\tNo. epoch: 171/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.695%\n",
      "\n",
      "\n",
      "\tNo. epoch: 172/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.721%\n",
      "\n",
      "\n",
      "\tNo. epoch: 173/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.682%\n",
      "\n",
      "\n",
      "\tNo. epoch: 174/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.682%\n",
      "\n",
      "\n",
      "\tNo. epoch: 175/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.695%\n",
      "\n",
      "\n",
      "\tNo. epoch: 176/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.689%\n",
      "\n",
      "\n",
      "\tNo. epoch: 177/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.67%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNo. epoch: 178/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.682%\n",
      "\n",
      "\n",
      "\tNo. epoch: 179/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.994%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.619%\n",
      "\n",
      "\n",
      "\tNo. epoch: 180/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.965%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.657%\n",
      "\n",
      "\n",
      "\tNo. epoch: 181/200                   \n",
      "\t   Train Loss: 0.284 | Train Acc: 99.319%\n",
      "\t    Val. Loss: 0.294 |  Val. Acc: 96.945%\n",
      "\n",
      "\n",
      "\tNo. epoch: 182/200                   \n",
      "\t   Train Loss: 0.289 | Train Acc: 98.358%\n",
      "\t    Val. Loss: 0.288 |  Val. Acc: 99.06%\n",
      "\n",
      "\n",
      "\tNo. epoch: 183/200                   \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.749%\n",
      "\t    Val. Loss: 0.286 |  Val. Acc: 99.149%\n",
      "\n",
      "\n",
      "\tNo. epoch: 184/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.878%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.587%\n",
      "\n",
      "\n",
      "\tNo. epoch: 185/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.974%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.638%\n",
      "\n",
      "\n",
      "\tNo. epoch: 186/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.99%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.663%\n",
      "\n",
      "\n",
      "\tNo. epoch: 187/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.992%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.657%\n",
      "\n",
      "\n",
      "\tNo. epoch: 188/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.983%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.676%\n",
      "\n",
      "\n",
      "\tNo. epoch: 189/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.994%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.657%\n",
      "\n",
      "\n",
      "\tNo. epoch: 190/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.978%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.651%\n",
      "\n",
      "\n",
      "\tNo. epoch: 191/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.81%\n",
      "\t    Val. Loss: 0.295 |  Val. Acc: 98.139%\n",
      "\n",
      "\n",
      "\tNo. epoch: 192/200                   \n",
      "\t   Train Loss: 0.288 | Train Acc: 98.275%\n",
      "\t    Val. Loss: 0.287 |  Val. Acc: 98.952%\n",
      "\n",
      "\n",
      "\tNo. epoch: 193/200                   \n",
      "\t   Train Loss: 0.283 | Train Acc: 99.734%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.555%\n",
      "\n",
      "\n",
      "\tNo. epoch: 194/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.949%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.606%\n",
      "\n",
      "\n",
      "\tNo. epoch: 195/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.966%\n",
      "\t    Val. Loss: 0.284 |  Val. Acc: 99.67%\n",
      "\n",
      "\n",
      "\tNo. epoch: 196/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.975%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.663%\n",
      "\n",
      "\n",
      "\tNo. epoch: 197/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.983%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.708%\n",
      "\n",
      "\n",
      "\tNo. epoch: 198/200                   \n",
      "\t   Train Loss: 0.282 | Train Acc: 99.986%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.708%\n",
      "\n",
      "\n",
      "\tNo. epoch: 199/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.987%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.695%\n",
      "\n",
      "\n",
      "\tNo. epoch: 200/200                   \n",
      "\t   Train Loss: 0.281 | Train Acc: 99.987%\n",
      "\t    Val. Loss: 0.285 |  Val. Acc: 99.701%\n",
      "\n",
      "\n",
      "Wall time: 40min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set seed.\n",
    "torch.manual_seed(1997)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# instantiate the model\n",
    "l2_MultiheadAttentionClassifier = MultiheadAttentionClassifier(\n",
    "    n_classes=1,\n",
    "    num_embeddings=len(TEXT.vocab),\n",
    "    embedding_dim=256,\n",
    "    num_layers=2,\n",
    "    heads=8,\n",
    "    device=\"cuda\",\n",
    "    augmentation_factor=4,\n",
    "    dropout=0,\n",
    "    max_length=fix_length,\n",
    "    pad_idx=TEXT.vocab.stoi[TEXT.pad_token]).to(\"cuda\")\n",
    "\n",
    "print(l2_MultiheadAttentionClassifier, \"\\n\")\n",
    "print(f'The model has {count_parameters(l2_MultiheadAttentionClassifier):,} trainable parameters.', \"\\n\")\n",
    "\n",
    "# Let us use Adam.\n",
    "OPT = optim.Adam(l2_MultiheadAttentionClassifier.parameters(), lr=0.0001)\n",
    "\n",
    "# Specify criterion.\n",
    "CRIT = nn.PoissonNLLLoss()\n",
    "\n",
    "MultiheadAttentionClassifier_train(l2_MultiheadAttentionClassifier,\n",
    "                                   train_iterator,\n",
    "                                   valid_iterator,\n",
    "                                   n_epochs,\n",
    "                                   OPT,\n",
    "                                   CRIT,\n",
    "                                   accuracy_function=poisson_accuracy,\n",
    "                                   save=True,\n",
    "                                   saving_path=\"model_bck/l2_MultiheadAttentionClassifier_lr0.0001_B384_D0_GC_false.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4deeb",
   "metadata": {},
   "source": [
    "# Normal label 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f13cd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  include_lengths=True, fix_length=fix_length)\n",
    "\n",
    "LABEL = data.Field(sequential=False, dtype=torch.float, batch_first=True,\n",
    "                   use_vocab=False, preprocessing=float)\n",
    "\n",
    "fields = [('text', TEXT), (None, None), (None, None),\n",
    "          ('label', LABEL), (None, None)]\n",
    "\n",
    "training_data = data.TabularDataset(path=path_to_data,\n",
    "                                    format=format_,\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=split_ratio,\n",
    "                                             random_state=random.seed(seed))\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=min_freq_words)\n",
    "\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            sort_key=lambda x: len(x.text),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b593c5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiheadAttentionClassifier(\n",
      "  (encoder): Encoder(\n",
      "    (tok_embedding): Embedding(15, 256)\n",
      "    (pos_embedding): Embedding(22, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadScaledDotProductAttention(\n",
      "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (LayerNormalization): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (LayerNormalization1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (LayerNormalization2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (linear_augmentation): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadScaledDotProductAttention(\n",
      "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (LayerNormalization): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (LayerNormalization1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (LayerNormalization2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (linear_augmentation): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (linear3): Linear(in_features=256, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "The model has 1,721,879 trainable parameters. \n",
      "\n",
      "\tNo. epoch: 1/200                     \n",
      "\t   Train Loss: 14388.211\n",
      "\t    Val. Loss: 14064.2\n",
      "\n",
      "\n",
      "\tNo. epoch: 2/200                     \n",
      "\t   Train Loss: 13530.587\n",
      "\t    Val. Loss: 12856.655\n",
      "\n",
      "\n",
      "\tNo. epoch: 3/200                     \n",
      "\t   Train Loss: 11864.662\n",
      "\t    Val. Loss: 10724.822\n",
      "\n",
      "\n",
      "\tNo. epoch: 4/200                     \n",
      "\t   Train Loss: 9328.76\n",
      "\t    Val. Loss: 7846.201\n",
      "\n",
      "\n",
      "\tNo. epoch: 5/200                     \n",
      "\t   Train Loss: 6328.609\n",
      "\t    Val. Loss: 4833.015\n",
      "\n",
      "\n",
      "\tNo. epoch: 6/200                     \n",
      "\t   Train Loss: 3535.36\n",
      "\t    Val. Loss: 2374.598\n",
      "\n",
      "\n",
      "\tNo. epoch: 7/200                     \n",
      "\t   Train Loss: 1558.921\n",
      "\t    Val. Loss: 902.234\n",
      "\n",
      "\n",
      "\tNo. epoch: 8/200                     \n",
      "\t   Train Loss: 533.399\n",
      "\t    Val. Loss: 274.293\n",
      "\n",
      "\n",
      "\tNo. epoch: 9/200                     \n",
      "\t   Train Loss: 167.264\n",
      "\t    Val. Loss: 101.185\n",
      "\n",
      "\n",
      "\tNo. epoch: 10/200                    \n",
      "\t   Train Loss: 81.557\n",
      "\t    Val. Loss: 70.112\n",
      "\n",
      "\n",
      "\tNo. epoch: 11/200                    \n",
      "\t   Train Loss: 69.058\n",
      "\t    Val. Loss: 61.33\n",
      "\n",
      "\n",
      "\tNo. epoch: 12/200                    \n",
      "\t   Train Loss: 49.619\n",
      "\t    Val. Loss: 35.26\n",
      "\n",
      "\n",
      "\tNo. epoch: 13/200                    \n",
      "\t   Train Loss: 29.115\n",
      "\t    Val. Loss: 19.539\n",
      "\n",
      "\n",
      "\tNo. epoch: 14/200                    \n",
      "\t   Train Loss: 17.616\n",
      "\t    Val. Loss: 11.988\n",
      "\n",
      "\n",
      "\tNo. epoch: 15/200                    \n",
      "\t   Train Loss: 9.196\n",
      "\t    Val. Loss: 5.863\n",
      "\n",
      "\n",
      "\tNo. epoch: 16/200                    \n",
      "\t   Train Loss: 4.554\n",
      "\t    Val. Loss: 3.793\n",
      "\n",
      "\n",
      "\tNo. epoch: 17/200                    \n",
      "\t   Train Loss: 2.97\n",
      "\t    Val. Loss: 2.776\n",
      "\n",
      "\n",
      "\tNo. epoch: 18/200                    \n",
      "\t   Train Loss: 2.113\n",
      "\t    Val. Loss: 2.144\n",
      "\n",
      "\n",
      "\tNo. epoch: 19/200                    \n",
      "\t   Train Loss: 1.62\n",
      "\t    Val. Loss: 1.719\n",
      "\n",
      "\n",
      "\tNo. epoch: 20/200                    \n",
      "\t   Train Loss: 1.25\n",
      "\t    Val. Loss: 1.34\n",
      "\n",
      "\n",
      "\tNo. epoch: 21/200                    \n",
      "\t   Train Loss: 1.093\n",
      "\t    Val. Loss: 1.267\n",
      "\n",
      "\n",
      "\tNo. epoch: 22/200                    \n",
      "\t   Train Loss: 1.095\n",
      "\t    Val. Loss: 1.09\n",
      "\n",
      "\n",
      "\tNo. epoch: 23/200                    \n",
      "\t   Train Loss: 1.809\n",
      "\t    Val. Loss: 2.671\n",
      "\n",
      "\n",
      "\tNo. epoch: 24/200                    \n",
      "\t   Train Loss: 3.454\n",
      "\t    Val. Loss: 5.402\n",
      "\n",
      "\n",
      "\tNo. epoch: 25/200                    \n",
      "\t   Train Loss: 1.46\n",
      "\t    Val. Loss: 1.009\n",
      "\n",
      "\n",
      "\tNo. epoch: 26/200                    \n",
      "\t   Train Loss: 0.78\n",
      "\t    Val. Loss: 0.771\n",
      "\n",
      "\n",
      "\tNo. epoch: 27/200                    \n",
      "\t   Train Loss: 0.572\n",
      "\t    Val. Loss: 1.027\n",
      "\n",
      "\n",
      "\tNo. epoch: 28/200                    \n",
      "\t   Train Loss: 0.665\n",
      "\t    Val. Loss: 0.547\n",
      "\n",
      "\n",
      "\tNo. epoch: 29/200                    \n",
      "\t   Train Loss: 0.427\n",
      "\t    Val. Loss: 0.535\n",
      "\n",
      "\n",
      "\tNo. epoch: 30/200                    \n",
      "\t   Train Loss: 0.483\n",
      "\t    Val. Loss: 0.679\n",
      "\n",
      "\n",
      "\tNo. epoch: 31/200                    \n",
      "\t   Train Loss: 0.589\n",
      "\t    Val. Loss: 0.511\n",
      "\n",
      "\n",
      "\tNo. epoch: 32/200                    \n",
      "\t   Train Loss: 1.193\n",
      "\t    Val. Loss: 1.145\n",
      "\n",
      "\n",
      "\tNo. epoch: 33/200                    \n",
      "\t   Train Loss: 0.714\n",
      "\t    Val. Loss: 0.56\n",
      "\n",
      "\n",
      "\tNo. epoch: 34/200                    \n",
      "\t   Train Loss: 0.428\n",
      "\t    Val. Loss: 0.444\n",
      "\n",
      "\n",
      "\tNo. epoch: 35/200                    \n",
      "\t   Train Loss: 0.382\n",
      "\t    Val. Loss: 0.817\n",
      "\n",
      "\n",
      "\tNo. epoch: 36/200                    \n",
      "\t   Train Loss: 1.08\n",
      "\t    Val. Loss: 0.869\n",
      "\n",
      "\n",
      "\tNo. epoch: 37/200                    \n",
      "\t   Train Loss: 1.025\n",
      "\t    Val. Loss: 0.529\n",
      "\n",
      "\n",
      "\tNo. epoch: 38/200                    \n",
      "\t   Train Loss: 0.498\n",
      "\t    Val. Loss: 0.433\n",
      "\n",
      "\n",
      "\tNo. epoch: 39/200                    \n",
      "\t   Train Loss: 0.431\n",
      "\t    Val. Loss: 0.293\n",
      "\n",
      "\n",
      "\tNo. epoch: 40/200                    \n",
      "\t   Train Loss: 0.217\n",
      "\t    Val. Loss: 0.267\n",
      "\n",
      "\n",
      "\tNo. epoch: 41/200                    \n",
      "\t   Train Loss: 0.195\n",
      "\t    Val. Loss: 0.248\n",
      "\n",
      "\n",
      "\tNo. epoch: 42/200                    \n",
      "\t   Train Loss: 0.217\n",
      "\t    Val. Loss: 0.299\n",
      "\n",
      "\n",
      "\tNo. epoch: 43/200                    \n",
      "\t   Train Loss: 0.584\n",
      "\t    Val. Loss: 0.411\n",
      "\n",
      "\n",
      "\tNo. epoch: 44/200                    \n",
      "\t   Train Loss: 0.766\n",
      "\t    Val. Loss: 0.804\n",
      "\n",
      "\n",
      "\tNo. epoch: 45/200                    \n",
      "\t   Train Loss: 0.26\n",
      "\t    Val. Loss: 0.262\n",
      "\n",
      "\n",
      "\tNo. epoch: 46/200                    \n",
      "\t   Train Loss: 0.45\n",
      "\t    Val. Loss: 0.552\n",
      "\n",
      "\n",
      "\tNo. epoch: 47/200                    \n",
      "\t   Train Loss: 0.722\n",
      "\t    Val. Loss: 0.408\n",
      "\n",
      "\n",
      "\tNo. epoch: 48/200                    \n",
      "\t   Train Loss: 0.785\n",
      "\t    Val. Loss: 0.252\n",
      "\n",
      "\n",
      "\tNo. epoch: 49/200                    \n",
      "\t   Train Loss: 0.321\n",
      "\t    Val. Loss: 0.431\n",
      "\n",
      "\n",
      "\tNo. epoch: 50/200                    \n",
      "\t   Train Loss: 0.629\n",
      "\t    Val. Loss: 0.648\n",
      "\n",
      "\n",
      "\tNo. epoch: 51/200                    \n",
      "\t   Train Loss: 0.391\n",
      "\t    Val. Loss: 0.333\n",
      "\n",
      "\n",
      "\tNo. epoch: 52/200                    \n",
      "\t   Train Loss: 0.782\n",
      "\t    Val. Loss: 0.379\n",
      "\n",
      "\n",
      "\tNo. epoch: 53/200                    \n",
      "\t   Train Loss: 0.338\n",
      "\t    Val. Loss: 0.2\n",
      "\n",
      "\n",
      "\tNo. epoch: 54/200                    \n",
      "\t   Train Loss: 0.137\n",
      "\t    Val. Loss: 0.169\n",
      "\n",
      "\n",
      "\tNo. epoch: 55/200                    \n",
      "\t   Train Loss: 0.12\n",
      "\t    Val. Loss: 0.162\n",
      "\n",
      "\n",
      "\tNo. epoch: 56/200                    \n",
      "\t   Train Loss: 0.113\n",
      "\t    Val. Loss: 0.162\n",
      "\n",
      "\n",
      "\tNo. epoch: 57/200                    \n",
      "\t   Train Loss: 0.118\n",
      "\t    Val. Loss: 0.222\n",
      "\n",
      "\n",
      "\tNo. epoch: 58/200                    \n",
      "\t   Train Loss: 0.162\n",
      "\t    Val. Loss: 0.14\n",
      "\n",
      "\n",
      "\tNo. epoch: 59/200                    \n",
      "\t   Train Loss: 0.162\n",
      "\t    Val. Loss: 0.331\n",
      "\n",
      "\n",
      "\tNo. epoch: 60/200                    \n",
      "\t   Train Loss: 0.836\n",
      "\t    Val. Loss: 0.851\n",
      "\n",
      "\n",
      "\tNo. epoch: 61/200                    \n",
      "\t   Train Loss: 0.867\n",
      "\t    Val. Loss: 1.253\n",
      "\n",
      "\n",
      "\tNo. epoch: 62/200                    \n",
      "\t   Train Loss: 0.71\n",
      "\t    Val. Loss: 0.19\n",
      "\n",
      "\n",
      "\tNo. epoch: 63/200                    \n",
      "\t   Train Loss: 0.155\n",
      "\t    Val. Loss: 0.152\n",
      "\n",
      "\n",
      "\tNo. epoch: 64/200                    \n",
      "\t   Train Loss: 0.1\n",
      "\t    Val. Loss: 0.126\n",
      "\n",
      "\n",
      "\tNo. epoch: 65/200                    \n",
      "\t   Train Loss: 0.087\n",
      "\t    Val. Loss: 0.122\n",
      "\n",
      "\n",
      "\tNo. epoch: 66/200                    \n",
      "\t   Train Loss: 0.091\n",
      "\t    Val. Loss: 0.15\n",
      "\n",
      "\n",
      "\tNo. epoch: 67/200                    \n",
      "\t   Train Loss: 0.103\n",
      "\t    Val. Loss: 0.188\n",
      "\n",
      "\n",
      "\tNo. epoch: 68/200                    \n",
      "\t   Train Loss: 0.161\n",
      "\t    Val. Loss: 0.711\n",
      "\n",
      "\n",
      "\tNo. epoch: 69/200                    \n",
      "\t   Train Loss: 1.663\n",
      "\t    Val. Loss: 0.772\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNo. epoch: 70/200                    \n",
      "\t   Train Loss: 0.451\n",
      "\t    Val. Loss: 0.133\n",
      "\n",
      "\n",
      "\tNo. epoch: 71/200                    \n",
      "\t   Train Loss: 0.083\n",
      "\t    Val. Loss: 0.111\n",
      "\n",
      "\n",
      "\tNo. epoch: 72/200                    \n",
      "\t   Train Loss: 0.073\n",
      "\t    Val. Loss: 0.104\n",
      "\n",
      "\n",
      "\tNo. epoch: 73/200                    \n",
      "\t   Train Loss: 0.067\n",
      "\t    Val. Loss: 0.1\n",
      "\n",
      "\n",
      "\tNo. epoch: 74/200                    \n",
      "\t   Train Loss: 0.065\n",
      "\t    Val. Loss: 0.092\n",
      "\n",
      "\n",
      "\tNo. epoch: 75/200                    \n",
      "\t   Train Loss: 0.062\n",
      "\t    Val. Loss: 0.092\n",
      "\n",
      "\n",
      "\tNo. epoch: 76/200                    \n",
      "\t   Train Loss: 0.062\n",
      "\t    Val. Loss: 0.101\n",
      "\n",
      "\n",
      "\tNo. epoch: 77/200                    \n",
      "\t   Train Loss: 0.093\n",
      "\t    Val. Loss: 0.169\n",
      "\n",
      "\n",
      "\tNo. epoch: 78/200                    \n",
      "\t   Train Loss: 0.176\n",
      "\t    Val. Loss: 0.817\n",
      "\n",
      "\n",
      "\tNo. epoch: 79/200                    \n",
      "\t   Train Loss: 0.25\n",
      "\t    Val. Loss: 0.103\n",
      "\n",
      "\n",
      "\tNo. epoch: 80/200                    \n",
      "\t   Train Loss: 0.178\n",
      "\t    Val. Loss: 0.297\n",
      "\n",
      "\n",
      "\tNo. epoch: 81/200                    \n",
      "\t   Train Loss: 0.947\n",
      "\t    Val. Loss: 0.35\n",
      "\n",
      "\n",
      "\tNo. epoch: 82/200                    \n",
      "\t   Train Loss: 0.57\n",
      "\t    Val. Loss: 0.301\n",
      "\n",
      "\n",
      "\tNo. epoch: 83/200                    \n",
      "\t   Train Loss: 0.088\n",
      "\t    Val. Loss: 0.099\n",
      "\n",
      "\n",
      "\tNo. epoch: 84/200                    \n",
      "\t   Train Loss: 0.056\n",
      "\t    Val. Loss: 0.081\n",
      "\n",
      "\n",
      "\tNo. epoch: 85/200                    \n",
      "\t   Train Loss: 0.049\n",
      "\t    Val. Loss: 0.077\n",
      "\n",
      "\n",
      "\tNo. epoch: 86/200                    \n",
      "\t   Train Loss: 0.046\n",
      "\t    Val. Loss: 0.074\n",
      "\n",
      "\n",
      "\tNo. epoch: 87/200                    \n",
      "\t   Train Loss: 0.044\n",
      "\t    Val. Loss: 0.074\n",
      "\n",
      "\n",
      "\tNo. epoch: 88/200                    \n",
      "\t   Train Loss: 0.044\n",
      "\t    Val. Loss: 0.076\n",
      "\n",
      "\n",
      "\tNo. epoch: 89/200                    \n",
      "\t   Train Loss: 0.06\n",
      "\t    Val. Loss: 0.086\n",
      "\n",
      "\n",
      "\tNo. epoch: 90/200                    \n",
      "\t   Train Loss: 0.153\n",
      "\t    Val. Loss: 0.217\n",
      "\n",
      "\n",
      "\tNo. epoch: 91/200                    \n",
      "\t   Train Loss: 0.182\n",
      "\t    Val. Loss: 0.351\n",
      "\n",
      "\n",
      "\tNo. epoch: 92/200                    \n",
      "\t   Train Loss: 1.368\n",
      "\t    Val. Loss: 0.389\n",
      "\n",
      "\n",
      "\tNo. epoch: 93/200                    \n",
      "\t   Train Loss: 0.287\n",
      "\t    Val. Loss: 0.092\n",
      "\n",
      "\n",
      "\tNo. epoch: 94/200                    \n",
      "\t   Train Loss: 0.053\n",
      "\t    Val. Loss: 0.079\n",
      "\n",
      "\n",
      "\tNo. epoch: 95/200                    \n",
      "\t   Train Loss: 0.046\n",
      "\t    Val. Loss: 0.072\n",
      "\n",
      "\n",
      "\tNo. epoch: 96/200                    \n",
      "\t   Train Loss: 0.041\n",
      "\t    Val. Loss: 0.068\n",
      "\n",
      "\n",
      "\tNo. epoch: 97/200                    \n",
      "\t   Train Loss: 0.038\n",
      "\t    Val. Loss: 0.065\n",
      "\n",
      "\n",
      "\tNo. epoch: 98/200                    \n",
      "\t   Train Loss: 0.036\n",
      "\t    Val. Loss: 0.065\n",
      "\n",
      "\n",
      "\tNo. epoch: 99/200                    \n",
      "\t   Train Loss: 0.047\n",
      "\t    Val. Loss: 0.067\n",
      "\n",
      "\n",
      "\tNo. epoch: 100/200                   \n",
      "\t   Train Loss: 0.037\n",
      "\t    Val. Loss: 0.065\n",
      "\n",
      "\n",
      "\tNo. epoch: 101/200                   \n",
      "\t   Train Loss: 0.034\n",
      "\t    Val. Loss: 0.058\n",
      "\n",
      "\n",
      "\tNo. epoch: 102/200                   \n",
      "\t   Train Loss: 0.038\n",
      "\t    Val. Loss: 0.064\n",
      "\n",
      "\n",
      "\tNo. epoch: 103/200                   \n",
      "\t   Train Loss: 0.103\n",
      "\t    Val. Loss: 0.092\n",
      "\n",
      "\n",
      "\tNo. epoch: 104/200                   \n",
      "\t   Train Loss: 0.806\n",
      "\t    Val. Loss: 1.599\n",
      "\n",
      "\n",
      "\tNo. epoch: 105/200                   \n",
      "\t   Train Loss: 0.398\n",
      "\t    Val. Loss: 0.122\n",
      "\n",
      "\n",
      "\tNo. epoch: 106/200                   \n",
      "\t   Train Loss: 0.076\n",
      "\t    Val. Loss: 0.079\n",
      "\n",
      "\n",
      "\tNo. epoch: 107/200                   \n",
      "\t   Train Loss: 0.042\n",
      "\t    Val. Loss: 0.062\n",
      "\n",
      "\n",
      "\tNo. epoch: 108/200                   \n",
      "\t   Train Loss: 0.035\n",
      "\t    Val. Loss: 0.06\n",
      "\n",
      "\n",
      "\tNo. epoch: 109/200                   \n",
      "\t   Train Loss: 0.049\n",
      "\t    Val. Loss: 0.129\n",
      "\n",
      "\n",
      "\tNo. epoch: 110/200                   \n",
      "\t   Train Loss: 0.035\n",
      "\t    Val. Loss: 0.061\n",
      "\n",
      "\n",
      "\tNo. epoch: 111/200                   \n",
      "\t   Train Loss: 0.13\n",
      "\t    Val. Loss: 0.355\n",
      "\n",
      "\n",
      "\tNo. epoch: 112/200                   \n",
      "\t   Train Loss: 0.254\n",
      "\t    Val. Loss: 0.144\n",
      "\n",
      "\n",
      "\tNo. epoch: 113/200                   \n",
      "\t   Train Loss: 0.434\n",
      "\t    Val. Loss: 0.277\n",
      "\n",
      "\n",
      "\tNo. epoch: 114/200                   \n",
      "\t   Train Loss: 0.589\n",
      "\t    Val. Loss: 0.141\n",
      "\n",
      "\n",
      "\tNo. epoch: 115/200                   \n",
      "\t   Train Loss: 0.139\n",
      "\t    Val. Loss: 0.083\n",
      "\n",
      "\n",
      "\tNo. epoch: 116/200                   \n",
      "\t   Train Loss: 0.044\n",
      "\t    Val. Loss: 0.059\n",
      "\n",
      "\n",
      "\tNo. epoch: 117/200                   \n",
      "\t   Train Loss: 0.031\n",
      "\t    Val. Loss: 0.056\n",
      "\n",
      "\n",
      "\tNo. epoch: 118/200                   \n",
      "\t   Train Loss: 0.028\n",
      "\t    Val. Loss: 0.053\n",
      "\n",
      "\n",
      "\tNo. epoch: 119/200                   \n",
      "\t   Train Loss: 0.026\n",
      "\t    Val. Loss: 0.051\n",
      "\n",
      "\n",
      "\tNo. epoch: 120/200                   \n",
      "\t   Train Loss: 0.026\n",
      "\t    Val. Loss: 0.05\n",
      "\n",
      "\n",
      "\tNo. epoch: 121/200                   \n",
      "\t   Train Loss: 0.024\n",
      "\t    Val. Loss: 0.049\n",
      "\n",
      "\n",
      "\tNo. epoch: 122/200                   \n",
      "\t   Train Loss: 0.023\n",
      "\t    Val. Loss: 0.048\n",
      "\n",
      "\n",
      "\tNo. epoch: 123/200                   \n",
      "\t   Train Loss: 0.026\n",
      "\t    Val. Loss: 0.057\n",
      "\n",
      "\n",
      "\tNo. epoch: 124/200                   \n",
      "\t   Train Loss: 0.061\n",
      "\t    Val. Loss: 0.412\n",
      "\n",
      "\n",
      "\tNo. epoch: 125/200                   \n",
      "\t   Train Loss: 0.393\n",
      "\t    Val. Loss: 0.524\n",
      "\n",
      "\n",
      "\tNo. epoch: 126/200                   \n",
      "\t   Train Loss: 0.719\n",
      "\t    Val. Loss: 0.391\n",
      "\n",
      "\n",
      "\tNo. epoch: 127/200                   \n",
      "\t   Train Loss: 0.18\n",
      "\t    Val. Loss: 0.062\n",
      "\n",
      "\n",
      "\tNo. epoch: 128/200                   \n",
      "\t   Train Loss: 0.034\n",
      "\t    Val. Loss: 0.053\n",
      "\n",
      "\n",
      "\tNo. epoch: 129/200                   \n",
      "\t   Train Loss: 0.026\n",
      "\t    Val. Loss: 0.051\n",
      "\n",
      "\n",
      "\tNo. epoch: 130/200                   \n",
      "\t   Train Loss: 0.023\n",
      "\t    Val. Loss: 0.048\n",
      "\n",
      "\n",
      "\tNo. epoch: 131/200                   \n",
      "\t   Train Loss: 0.022\n",
      "\t    Val. Loss: 0.046\n",
      "\n",
      "\n",
      "\tNo. epoch: 132/200                   \n",
      "\t   Train Loss: 0.02\n",
      "\t    Val. Loss: 0.045\n",
      "\n",
      "\n",
      "\tNo. epoch: 133/200                   \n",
      "\t   Train Loss: 0.02\n",
      "\t    Val. Loss: 0.047\n",
      "\n",
      "\n",
      "\tNo. epoch: 134/200                   \n",
      "\t   Train Loss: 0.026\n",
      "\t    Val. Loss: 0.047\n",
      "\n",
      "\n",
      "\tNo. epoch: 135/200                   \n",
      "\t   Train Loss: 0.071\n",
      "\t    Val. Loss: 0.092\n",
      "\n",
      "\n",
      "\tNo. epoch: 136/200                   \n",
      "\t   Train Loss: 0.271\n",
      "\t    Val. Loss: 0.147\n",
      "\n",
      "\n",
      "\tNo. epoch: 137/200                   \n",
      "\t   Train Loss: 0.423\n",
      "\t    Val. Loss: 0.446\n",
      "\n",
      "\n",
      "\tNo. epoch: 138/200                   \n",
      "\t   Train Loss: 0.237\n",
      "\t    Val. Loss: 0.07\n",
      "\n",
      "\n",
      "\tNo. epoch: 139/200                   \n",
      "\t   Train Loss: 0.038\n",
      "\t    Val. Loss: 0.055\n",
      "\n",
      "\n",
      "\tNo. epoch: 140/200                   \n",
      "\t   Train Loss: 0.026\n",
      "\t    Val. Loss: 0.086\n",
      "\n",
      "\n",
      "\tNo. epoch: 141/200                   \n",
      "\t   Train Loss: 0.037\n",
      "\t    Val. Loss: 0.048\n",
      "\n",
      "\n",
      "\tNo. epoch: 142/200                   \n",
      "\t   Train Loss: 0.035\n",
      "\t    Val. Loss: 0.053\n",
      "\n",
      "\n",
      "\tNo. epoch: 143/200                   \n",
      "\t   Train Loss: 0.041\n",
      "\t    Val. Loss: 0.096\n",
      "\n",
      "\n",
      "\tNo. epoch: 144/200                   \n",
      "\t   Train Loss: 0.043\n",
      "\t    Val. Loss: 0.069\n",
      "\n",
      "\n",
      "\tNo. epoch: 145/200                   \n",
      "\t   Train Loss: 0.106\n",
      "\t    Val. Loss: 0.065\n",
      "\n",
      "\n",
      "\tNo. epoch: 146/200                   \n",
      "\t   Train Loss: 0.092\n",
      "\t    Val. Loss: 0.143\n",
      "\n",
      "\n",
      "\tNo. epoch: 147/200                   \n",
      "\t   Train Loss: 0.08\n",
      "\t    Val. Loss: 0.072\n",
      "\n",
      "\n",
      "\tNo. epoch: 148/200                   \n",
      "\t   Train Loss: 0.066\n",
      "\t    Val. Loss: 0.074\n",
      "\n",
      "\n",
      "\tNo. epoch: 149/200                   \n",
      "\t   Train Loss: 0.139\n",
      "\t    Val. Loss: 0.373\n",
      "\n",
      "\n",
      "\tNo. epoch: 150/200                   \n",
      "\t   Train Loss: 0.408\n",
      "\t    Val. Loss: 0.115\n",
      "\n",
      "\n",
      "\tNo. epoch: 151/200                   \n",
      "\t   Train Loss: 0.067\n",
      "\t    Val. Loss: 0.051\n",
      "\n",
      "\n",
      "\tNo. epoch: 152/200                   \n",
      "\t   Train Loss: 0.022\n",
      "\t    Val. Loss: 0.041\n",
      "\n",
      "\n",
      "\tNo. epoch: 153/200                   \n",
      "\t   Train Loss: 0.018\n",
      "\t    Val. Loss: 0.039\n",
      "\n",
      "\n",
      "\tNo. epoch: 154/200                   \n",
      "\t   Train Loss: 0.017\n",
      "\t    Val. Loss: 0.038\n",
      "\n",
      "\n",
      "\tNo. epoch: 155/200                   \n",
      "\t   Train Loss: 0.015\n",
      "\t    Val. Loss: 0.037\n",
      "\n",
      "\n",
      "\tNo. epoch: 156/200                   \n",
      "\t   Train Loss: 0.015\n",
      "\t    Val. Loss: 0.037\n",
      "\n",
      "\n",
      "\tNo. epoch: 157/200                   \n",
      "\t   Train Loss: 0.014\n",
      "\t    Val. Loss: 0.037\n",
      "\n",
      "\n",
      "\tNo. epoch: 158/200                   \n",
      "\t   Train Loss: 0.049\n",
      "\t    Val. Loss: 0.067\n",
      "\n",
      "\n",
      "\tNo. epoch: 159/200                   \n",
      "\t   Train Loss: 0.148\n",
      "\t    Val. Loss: 0.417\n",
      "\n",
      "\n",
      "\tNo. epoch: 160/200                   \n",
      "\t   Train Loss: 0.437\n",
      "\t    Val. Loss: 0.092\n",
      "\n",
      "\n",
      "\tNo. epoch: 161/200                   \n",
      "\t   Train Loss: 0.083\n",
      "\t    Val. Loss: 0.09\n",
      "\n",
      "\n",
      "\tNo. epoch: 162/200                   \n",
      "\t   Train Loss: 0.056\n",
      "\t    Val. Loss: 0.085\n",
      "\n",
      "\n",
      "\tNo. epoch: 163/200                   \n",
      "\t   Train Loss: 0.058\n",
      "\t    Val. Loss: 0.068\n",
      "\n",
      "\n",
      "\tNo. epoch: 164/200                   \n",
      "\t   Train Loss: 0.035\n",
      "\t    Val. Loss: 0.048\n",
      "\n",
      "\n",
      "\tNo. epoch: 165/200                   \n",
      "\t   Train Loss: 0.025\n",
      "\t    Val. Loss: 0.039\n",
      "\n",
      "\n",
      "\tNo. epoch: 166/200                   \n",
      "\t   Train Loss: 0.016\n",
      "\t    Val. Loss: 0.036\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNo. epoch: 167/200                   \n",
      "\t   Train Loss: 0.014\n",
      "\t    Val. Loss: 0.035\n",
      "\n",
      "\n",
      "\tNo. epoch: 168/200                   \n",
      "\t   Train Loss: 0.014\n",
      "\t    Val. Loss: 0.037\n",
      "\n",
      "\n",
      "\tNo. epoch: 169/200                   \n",
      "\t   Train Loss: 0.013\n",
      "\t    Val. Loss: 0.036\n",
      "\n",
      "\n",
      "\tNo. epoch: 170/200                   \n",
      "\t   Train Loss: 0.055\n",
      "\t    Val. Loss: 0.664\n",
      "\n",
      "\n",
      "\tNo. epoch: 171/200                   \n",
      "\t   Train Loss: 0.296\n",
      "\t    Val. Loss: 0.046\n",
      "\n",
      "\n",
      "\tNo. epoch: 172/200                   \n",
      "\t   Train Loss: 0.019\n",
      "\t    Val. Loss: 0.042\n",
      "\n",
      "\n",
      "\tNo. epoch: 173/200                   \n",
      "\t   Train Loss: 0.016\n",
      "\t    Val. Loss: 0.036\n",
      "\n",
      "\n",
      "\tNo. epoch: 174/200                   \n",
      "\t   Train Loss: 0.034\n",
      "\t    Val. Loss: 0.071\n",
      "\n",
      "\n",
      "\tNo. epoch: 175/200                   \n",
      "\t   Train Loss: 0.195\n",
      "\t    Val. Loss: 0.356\n",
      "\n",
      "\n",
      "\tNo. epoch: 176/200                   \n",
      "\t   Train Loss: 0.308\n",
      "\t    Val. Loss: 0.094\n",
      "\n",
      "\n",
      "\tNo. epoch: 177/200                   \n",
      "\t   Train Loss: 0.04\n",
      "\t    Val. Loss: 0.038\n",
      "\n",
      "\n",
      "\tNo. epoch: 178/200                   \n",
      "\t   Train Loss: 0.016\n",
      "\t    Val. Loss: 0.035\n",
      "\n",
      "\n",
      "\tNo. epoch: 179/200                   \n",
      "\t   Train Loss: 0.014\n",
      "\t    Val. Loss: 0.036\n",
      "\n",
      "\n",
      "\tNo. epoch: 180/200                   \n",
      "\t   Train Loss: 0.016\n",
      "\t    Val. Loss: 0.034\n",
      "\n",
      "\n",
      "\tNo. epoch: 181/200                   \n",
      "\t   Train Loss: 0.015\n",
      "\t    Val. Loss: 0.053\n",
      "\n",
      "\n",
      "\tNo. epoch: 182/200                   \n",
      "\t   Train Loss: 0.031\n",
      "\t    Val. Loss: 0.046\n",
      "\n",
      "\n",
      "\tNo. epoch: 183/200                   \n",
      "\t   Train Loss: 0.071\n",
      "\t    Val. Loss: 0.213\n",
      "\n",
      "\n",
      "\tNo. epoch: 184/200                   \n",
      "\t   Train Loss: 0.174\n",
      "\t    Val. Loss: 0.066\n",
      "\n",
      "\n",
      "\tNo. epoch: 185/200                   \n",
      "\t   Train Loss: 0.071\n",
      "\t    Val. Loss: 0.05\n",
      "\n",
      "\n",
      "\tNo. epoch: 186/200                   \n",
      "\t   Train Loss: 0.056\n",
      "\t    Val. Loss: 0.17\n",
      "\n",
      "\n",
      "\tNo. epoch: 187/200                   \n",
      "\t   Train Loss: 0.054\n",
      "\t    Val. Loss: 0.056\n",
      "\n",
      "\n",
      "\tNo. epoch: 188/200                   \n",
      "\t   Train Loss: 0.075\n",
      "\t    Val. Loss: 0.04\n",
      "\n",
      "\n",
      "\tNo. epoch: 189/200                   \n",
      "\t   Train Loss: 0.019\n",
      "\t    Val. Loss: 0.034\n",
      "\n",
      "\n",
      "\tNo. epoch: 190/200                   \n",
      "\t   Train Loss: 0.018\n",
      "\t    Val. Loss: 0.035\n",
      "\n",
      "\n",
      "\tNo. epoch: 191/200                   \n",
      "\t   Train Loss: 0.016\n",
      "\t    Val. Loss: 0.086\n",
      "\n",
      "\n",
      "\tNo. epoch: 192/200                   \n",
      "\t   Train Loss: 0.125\n",
      "\t    Val. Loss: 0.167\n",
      "\n",
      "\n",
      "\tNo. epoch: 193/200                   \n",
      "\t   Train Loss: 0.197\n",
      "\t    Val. Loss: 0.172\n",
      "\n",
      "\n",
      "\tNo. epoch: 194/200                   \n",
      "\t   Train Loss: 0.161\n",
      "\t    Val. Loss: 0.048\n",
      "\n",
      "\n",
      "\tNo. epoch: 195/200                   \n",
      "\t   Train Loss: 0.021\n",
      "\t    Val. Loss: 0.034\n",
      "\n",
      "\n",
      "\tNo. epoch: 196/200                   \n",
      "\t   Train Loss: 0.013\n",
      "\t    Val. Loss: 0.033\n",
      "\n",
      "\n",
      "\tNo. epoch: 197/200                   \n",
      "\t   Train Loss: 0.012\n",
      "\t    Val. Loss: 0.031\n",
      "\n",
      "\n",
      "\tNo. epoch: 198/200                   \n",
      "\t   Train Loss: 0.011\n",
      "\t    Val. Loss: 0.033\n",
      "\n",
      "\n",
      "\tNo. epoch: 199/200                   \n",
      "\t   Train Loss: 0.011\n",
      "\t    Val. Loss: 0.032\n",
      "\n",
      "\n",
      "\tNo. epoch: 200/200                   \n",
      "\t   Train Loss: 0.011\n",
      "\t    Val. Loss: 0.034\n",
      "\n",
      "\n",
      "Wall time: 40min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set seed.\n",
    "torch.manual_seed(1997)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# instantiate the model\n",
    "l3_MultiheadAttentionClassifier = MultiheadAttentionClassifier(\n",
    "    n_classes=1,\n",
    "    num_embeddings=len(TEXT.vocab),\n",
    "    embedding_dim=256,\n",
    "    num_layers=2,\n",
    "    heads=8,\n",
    "    device=\"cuda\",\n",
    "    augmentation_factor=4,\n",
    "    dropout=0,\n",
    "    max_length=fix_length,\n",
    "    pad_idx=TEXT.vocab.stoi[TEXT.pad_token]).to(\"cuda\")\n",
    "\n",
    "print(l3_MultiheadAttentionClassifier, \"\\n\")\n",
    "print(f'The model has {count_parameters(l3_MultiheadAttentionClassifier):,} trainable parameters.', \"\\n\")\n",
    "\n",
    "# Let us use Adam.\n",
    "OPT = optim.Adam(l3_MultiheadAttentionClassifier.parameters(), lr=0.00001)\n",
    "\n",
    "# Specify criterion.\n",
    "CRIT = nn.MSELoss()\n",
    "\n",
    "MultiheadAttentionClassifier_train(l3_MultiheadAttentionClassifier,\n",
    "                                   train_iterator,\n",
    "                                   valid_iterator,\n",
    "                                   n_epochs,\n",
    "                                   OPT,\n",
    "                                   CRIT,\n",
    "                                   clipping_value=None,\n",
    "                                   accuracy_function=None,\n",
    "                                   save=True,\n",
    "                                   saving_path=\"model_bck/l3_MultiheadAttentionClassifier_lr0.0001_B384_D0_GC_false.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fa2ba",
   "metadata": {},
   "source": [
    "# Poisson label 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dccbb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  include_lengths=True, fix_length=fix_length)\n",
    "\n",
    "LABEL = data.Field(sequential=False, dtype=torch.float, batch_first=True,\n",
    "                   use_vocab=False, preprocessing=float)\n",
    "\n",
    "fields = [('text', TEXT), (None, None), (None, None),\n",
    "          (None, None), ('label', LABEL)]\n",
    "\n",
    "training_data = data.TabularDataset(path=path_to_data,\n",
    "                                    format=format_,\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=split_ratio,\n",
    "                                             random_state=random.seed(seed))\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=min_freq_words)\n",
    "\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            sort_key=lambda x: len(x.text),\n",
    "                                                            sort_within_batch=True,\n",
    "                                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f4c9861",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiheadAttentionClassifier(\n",
      "  (encoder): Encoder(\n",
      "    (tok_embedding): Embedding(15, 256)\n",
      "    (pos_embedding): Embedding(22, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadScaledDotProductAttention(\n",
      "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (LayerNormalization): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (LayerNormalization1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (LayerNormalization2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (linear_augmentation): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadScaledDotProductAttention(\n",
      "          (V): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (K): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (LayerNormalization): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (LayerNormalization1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (LayerNormalization2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (linear_augmentation): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (linear3): Linear(in_features=256, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "The model has 1,721,879 trainable parameters. \n",
      "\n",
      "\tNo. epoch: 1/200                     \n",
      "\t   Train Loss: 0.362 | Train Acc: 68.566%\n",
      "\t    Val. Loss: 0.181 |  Val. Acc: 83.386%\n",
      "\n",
      "\n",
      "\tNo. epoch: 2/200                     \n",
      "\t   Train Loss: 0.254 | Train Acc: 83.893%\n",
      "\t    Val. Loss: 0.173 |  Val. Acc: 82.447%\n",
      "\n",
      "\n",
      "\tNo. epoch: 3/200                     \n",
      "\t   Train Loss: 0.254 | Train Acc: 83.938%\n",
      "\t    Val. Loss: 0.172 |  Val. Acc: 83.974%\n",
      "\n",
      "\n",
      "\tNo. epoch: 4/200                     \n",
      "\t   Train Loss: 0.251 | Train Acc: 85.11%\n",
      "\t    Val. Loss: 0.17 |  Val. Acc: 84.131%\n",
      "\n",
      "\n",
      "\tNo. epoch: 5/200                     \n",
      "\t   Train Loss: 0.25 | Train Acc: 85.792%\n",
      "\t    Val. Loss: 0.169 |  Val. Acc: 85.332%\n",
      "\n",
      "\n",
      "\tNo. epoch: 6/200                     \n",
      "\t   Train Loss: 0.248 | Train Acc: 87.0%\n",
      "\t    Val. Loss: 0.167 |  Val. Acc: 86.577%\n",
      "\n",
      "\n",
      "\tNo. epoch: 7/200                     \n",
      "\t   Train Loss: 0.246 | Train Acc: 87.93%\n",
      "\t    Val. Loss: 0.164 |  Val. Acc: 88.489%\n",
      "\n",
      "\n",
      "\tNo. epoch: 8/200                     \n",
      "\t   Train Loss: 0.242 | Train Acc: 90.667%\n",
      "\t    Val. Loss: 0.163 |  Val. Acc: 90.348%\n",
      "\n",
      "\n",
      "\tNo. epoch: 9/200                     \n",
      "\t   Train Loss: 0.24 | Train Acc: 92.473%\n",
      "\t    Val. Loss: 0.164 |  Val. Acc: 89.104%\n",
      "\n",
      "\n",
      "\tNo. epoch: 10/200                    \n",
      "\t   Train Loss: 0.244 | Train Acc: 90.849%\n",
      "\t    Val. Loss: 0.165 |  Val. Acc: 87.374%\n",
      "\n",
      "\n",
      "\tNo. epoch: 11/200                    \n",
      "\t   Train Loss: 0.239 | Train Acc: 92.919%\n",
      "\t    Val. Loss: 0.158 |  Val. Acc: 93.886%\n",
      "\n",
      "\n",
      "\tNo. epoch: 12/200                    \n",
      "\t   Train Loss: 0.24 | Train Acc: 93.094%\n",
      "\t    Val. Loss: 0.16 |  Val. Acc: 91.881%\n",
      "\n",
      "\n",
      "\tNo. epoch: 13/200                    \n",
      "\t   Train Loss: 0.236 | Train Acc: 95.505%\n",
      "\t    Val. Loss: 0.156 |  Val. Acc: 95.474%\n",
      "\n",
      "\n",
      "\tNo. epoch: 14/200                    \n",
      "\t   Train Loss: 0.234 | Train Acc: 97.078%\n",
      "\t    Val. Loss: 0.155 |  Val. Acc: 95.788%\n",
      "\n",
      "\n",
      "\tNo. epoch: 15/200                    \n",
      "\t   Train Loss: 0.235 | Train Acc: 96.335%\n",
      "\t    Val. Loss: 0.156 |  Val. Acc: 95.123%\n",
      "\n",
      "\n",
      "\tNo. epoch: 16/200                    \n",
      "\t   Train Loss: 0.234 | Train Acc: 96.896%\n",
      "\t    Val. Loss: 0.155 |  Val. Acc: 96.247%\n",
      "\n",
      "\n",
      "\tNo. epoch: 17/200                    \n",
      "\t   Train Loss: 0.233 | Train Acc: 97.783%\n",
      "\t    Val. Loss: 0.156 |  Val. Acc: 95.265%\n",
      "\n",
      "\n",
      "\tNo. epoch: 18/200                    \n",
      "\t   Train Loss: 0.233 | Train Acc: 97.965%\n",
      "\t    Val. Loss: 0.153 |  Val. Acc: 97.298%\n",
      "\n",
      "\n",
      "\tNo. epoch: 19/200                    \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.913%\n",
      "\t    Val. Loss: 0.152 |  Val. Acc: 98.035%\n",
      "\n",
      "\n",
      "\tNo. epoch: 20/200                    \n",
      "\t   Train Loss: 0.232 | Train Acc: 99.208%\n",
      "\t    Val. Loss: 0.154 |  Val. Acc: 96.873%\n",
      "\n",
      "\n",
      "\tNo. epoch: 21/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.216%\n",
      "\t    Val. Loss: 0.152 |  Val. Acc: 97.704%\n",
      "\n",
      "\n",
      "\tNo. epoch: 22/200                    \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.82%\n",
      "\t    Val. Loss: 0.154 |  Val. Acc: 96.932%\n",
      "\n",
      "\n",
      "\tNo. epoch: 23/200                    \n",
      "\t   Train Loss: 0.232 | Train Acc: 99.153%\n",
      "\t    Val. Loss: 0.152 |  Val. Acc: 98.149%\n",
      "\n",
      "\n",
      "\tNo. epoch: 24/200                    \n",
      "\t   Train Loss: 0.232 | Train Acc: 99.092%\n",
      "\t    Val. Loss: 0.152 |  Val. Acc: 97.926%\n",
      "\n",
      "\n",
      "\tNo. epoch: 25/200                    \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.838%\n",
      "\t    Val. Loss: 0.152 |  Val. Acc: 98.428%\n",
      "\n",
      "\n",
      "\tNo. epoch: 26/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.708%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.804%\n",
      "\n",
      "\n",
      "\tNo. epoch: 27/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.745%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.765%\n",
      "\n",
      "\n",
      "\tNo. epoch: 28/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.813%\n",
      "\t    Val. Loss: 0.152 |  Val. Acc: 98.667%\n",
      "\n",
      "\n",
      "\tNo. epoch: 29/200                    \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.715%\n",
      "\t    Val. Loss: 0.155 |  Val. Acc: 95.956%\n",
      "\n",
      "\n",
      "\tNo. epoch: 30/200                    \n",
      "\t   Train Loss: 0.233 | Train Acc: 98.304%\n",
      "\t    Val. Loss: 0.153 |  Val. Acc: 97.597%\n",
      "\n",
      "\n",
      "\tNo. epoch: 31/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.589%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.647%\n",
      "\n",
      "\n",
      "\tNo. epoch: 32/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.861%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.88%\n",
      "\n",
      "\n",
      "\tNo. epoch: 33/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.017%\n",
      "\t    Val. Loss: 0.152 |  Val. Acc: 98.155%\n",
      "\n",
      "\n",
      "\tNo. epoch: 34/200                    \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.481%\n",
      "\t    Val. Loss: 0.153 |  Val. Acc: 98.1%\n",
      "\n",
      "\n",
      "\tNo. epoch: 35/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.592%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.644%\n",
      "\n",
      "\n",
      "\tNo. epoch: 36/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.929%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.924%\n",
      "\n",
      "\n",
      "\tNo. epoch: 37/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.962%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.854%\n",
      "\n",
      "\n",
      "\tNo. epoch: 38/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.975%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.94%\n",
      "\n",
      "\n",
      "\tNo. epoch: 39/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.982%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.956%\n",
      "\n",
      "\n",
      "\tNo. epoch: 40/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.984%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.898%\n",
      "\n",
      "\n",
      "\tNo. epoch: 41/200                    \n",
      "\t   Train Loss: 0.244 | Train Acc: 94.295%\n",
      "\t    Val. Loss: 0.158 |  Val. Acc: 93.102%\n",
      "\n",
      "\n",
      "\tNo. epoch: 42/200                    \n",
      "\t   Train Loss: 0.234 | Train Acc: 97.238%\n",
      "\t    Val. Loss: 0.152 |  Val. Acc: 98.209%\n",
      "\n",
      "\n",
      "\tNo. epoch: 43/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.605%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.74%\n",
      "\n",
      "\n",
      "\tNo. epoch: 44/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.869%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.94%\n",
      "\n",
      "\n",
      "\tNo. epoch: 45/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.939%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.01%\n",
      "\n",
      "\n",
      "\tNo. epoch: 46/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.964%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.962%\n",
      "\n",
      "\n",
      "\tNo. epoch: 47/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.964%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.946%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNo. epoch: 48/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.977%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.061%\n",
      "\n",
      "\n",
      "\tNo. epoch: 49/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.981%\n",
      "\n",
      "\n",
      "\tNo. epoch: 50/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.921%\n",
      "\t    Val. Loss: 0.152 |  Val. Acc: 98.38%\n",
      "\n",
      "\n",
      "\tNo. epoch: 51/200                    \n",
      "\t   Train Loss: 0.232 | Train Acc: 99.096%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.966%\n",
      "\n",
      "\n",
      "\tNo. epoch: 52/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.895%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.873%\n",
      "\n",
      "\n",
      "\tNo. epoch: 53/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.853%\n",
      "\t    Val. Loss: 0.153 |  Val. Acc: 98.098%\n",
      "\n",
      "\n",
      "\tNo. epoch: 54/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.376%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.137%\n",
      "\n",
      "\n",
      "\tNo. epoch: 55/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.979%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.22%\n",
      "\n",
      "\n",
      "\tNo. epoch: 56/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.994%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.268%\n",
      "\n",
      "\n",
      "\tNo. epoch: 57/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.172%\n",
      "\n",
      "\n",
      "\tNo. epoch: 58/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.951%\n",
      "\t    Val. Loss: 0.152 |  Val. Acc: 98.357%\n",
      "\n",
      "\n",
      "\tNo. epoch: 59/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.929%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.147%\n",
      "\n",
      "\n",
      "\tNo. epoch: 60/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.201%\n",
      "\n",
      "\n",
      "\tNo. epoch: 61/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.14%\n",
      "\n",
      "\n",
      "\tNo. epoch: 62/200                    \n",
      "\t   Train Loss: 0.233 | Train Acc: 97.789%\n",
      "\t    Val. Loss: 0.163 |  Val. Acc: 90.028%\n",
      "\n",
      "\n",
      "\tNo. epoch: 63/200                    \n",
      "\t   Train Loss: 0.233 | Train Acc: 97.762%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.689%\n",
      "\n",
      "\n",
      "\tNo. epoch: 64/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.892%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.106%\n",
      "\n",
      "\n",
      "\tNo. epoch: 65/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.969%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.157%\n",
      "\n",
      "\n",
      "\tNo. epoch: 66/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.992%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.109%\n",
      "\n",
      "\n",
      "\tNo. epoch: 67/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.029%\n",
      "\n",
      "\n",
      "\tNo. epoch: 68/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.204%\n",
      "\n",
      "\n",
      "\tNo. epoch: 69/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.163%\n",
      "\n",
      "\n",
      "\tNo. epoch: 70/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.109%\n",
      "\n",
      "\n",
      "\tNo. epoch: 71/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.969%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.083%\n",
      "\n",
      "\n",
      "\tNo. epoch: 72/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.981%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.278%\n",
      "\n",
      "\n",
      "\tNo. epoch: 73/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.973%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.192%\n",
      "\n",
      "\n",
      "\tNo. epoch: 74/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.992%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.147%\n",
      "\n",
      "\n",
      "\tNo. epoch: 75/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.965%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.045%\n",
      "\n",
      "\n",
      "\tNo. epoch: 76/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.975%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.778%\n",
      "\n",
      "\n",
      "\tNo. epoch: 77/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.895%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.173%\n",
      "\n",
      "\n",
      "\tNo. epoch: 78/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.534%\n",
      "\t    Val. Loss: 0.152 |  Val. Acc: 98.374%\n",
      "\n",
      "\n",
      "\tNo. epoch: 79/200                    \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.771%\n",
      "\t    Val. Loss: 0.153 |  Val. Acc: 97.53%\n",
      "\n",
      "\n",
      "\tNo. epoch: 80/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.842%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.153%\n",
      "\n",
      "\n",
      "\tNo. epoch: 81/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.994%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 98.984%\n",
      "\n",
      "\n",
      "\tNo. epoch: 82/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.016%\n",
      "\n",
      "\n",
      "\tNo. epoch: 83/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.064%\n",
      "\n",
      "\n",
      "\tNo. epoch: 84/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.118%\n",
      "\n",
      "\n",
      "\tNo. epoch: 85/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 98.997%\n",
      "\n",
      "\n",
      "\tNo. epoch: 86/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.045%\n",
      "\n",
      "\n",
      "\tNo. epoch: 87/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.239%\n",
      "\n",
      "\n",
      "\tNo. epoch: 88/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.88%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.071%\n",
      "\n",
      "\n",
      "\tNo. epoch: 89/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.994%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.131%\n",
      "\n",
      "\n",
      "\tNo. epoch: 90/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.08%\n",
      "\n",
      "\n",
      "\tNo. epoch: 91/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.926%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.057%\n",
      "\n",
      "\n",
      "\tNo. epoch: 92/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.074%\n",
      "\n",
      "\n",
      "\tNo. epoch: 93/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.984%\n",
      "\n",
      "\n",
      "\tNo. epoch: 94/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.994%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.226%\n",
      "\n",
      "\n",
      "\tNo. epoch: 95/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.204%\n",
      "\n",
      "\n",
      "\tNo. epoch: 96/200                    \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.198%\n",
      "\n",
      "\n",
      "\tNo. epoch: 97/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.715%\n",
      "\t    Val. Loss: 0.153 |  Val. Acc: 96.659%\n",
      "\n",
      "\n",
      "\tNo. epoch: 98/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.179%\n",
      "\t    Val. Loss: 0.153 |  Val. Acc: 98.673%\n",
      "\n",
      "\n",
      "\tNo. epoch: 99/200                    \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.743%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.038%\n",
      "\n",
      "\n",
      "\tNo. epoch: 100/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.951%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.105%\n",
      "\n",
      "\n",
      "\tNo. epoch: 101/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.191%\n",
      "\n",
      "\n",
      "\tNo. epoch: 102/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.258%\n",
      "\n",
      "\n",
      "\tNo. epoch: 103/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.245%\n",
      "\n",
      "\n",
      "\tNo. epoch: 104/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.185%\n",
      "\n",
      "\n",
      "\tNo. epoch: 105/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.258%\n",
      "\n",
      "\n",
      "\tNo. epoch: 106/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.179%\n",
      "\n",
      "\n",
      "\tNo. epoch: 107/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.281%\n",
      "\n",
      "\n",
      "\tNo. epoch: 108/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.185%\n",
      "\n",
      "\n",
      "\tNo. epoch: 109/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.886%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.997%\n",
      "\n",
      "\n",
      "\tNo. epoch: 110/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.933%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.099%\n",
      "\n",
      "\n",
      "\tNo. epoch: 111/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.994%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.21%\n",
      "\n",
      "\n",
      "\tNo. epoch: 112/200                   \n",
      "\t   Train Loss: 0.233 | Train Acc: 97.831%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.115%\n",
      "\n",
      "\n",
      "\tNo. epoch: 113/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.937%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.124%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNo. epoch: 114/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.996%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.131%\n",
      "\n",
      "\n",
      "\tNo. epoch: 115/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.15%\n",
      "\n",
      "\n",
      "\tNo. epoch: 116/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.076%\n",
      "\n",
      "\n",
      "\tNo. epoch: 117/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.083%\n",
      "\n",
      "\n",
      "\tNo. epoch: 118/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.083%\n",
      "\n",
      "\n",
      "\tNo. epoch: 119/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.137%\n",
      "\n",
      "\n",
      "\tNo. epoch: 120/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.083%\n",
      "\n",
      "\n",
      "\tNo. epoch: 121/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.089%\n",
      "\n",
      "\n",
      "\tNo. epoch: 122/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.137%\n",
      "\n",
      "\n",
      "\tNo. epoch: 123/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.917%\n",
      "\n",
      "\n",
      "\tNo. epoch: 124/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.975%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.118%\n",
      "\n",
      "\n",
      "\tNo. epoch: 125/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.258%\n",
      "\n",
      "\n",
      "\tNo. epoch: 126/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.204%\n",
      "\n",
      "\n",
      "\tNo. epoch: 127/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.252%\n",
      "\n",
      "\n",
      "\tNo. epoch: 128/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.198%\n",
      "\n",
      "\n",
      "\tNo. epoch: 129/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.204%\n",
      "\n",
      "\n",
      "\tNo. epoch: 130/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.137%\n",
      "\n",
      "\n",
      "\tNo. epoch: 131/200                   \n",
      "\t   Train Loss: 0.233 | Train Acc: 98.312%\n",
      "\t    Val. Loss: 0.153 |  Val. Acc: 97.547%\n",
      "\n",
      "\n",
      "\tNo. epoch: 132/200                   \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.232%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.054%\n",
      "\n",
      "\n",
      "\tNo. epoch: 133/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.982%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.083%\n",
      "\n",
      "\n",
      "\tNo. epoch: 134/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.15%\n",
      "\n",
      "\n",
      "\tNo. epoch: 135/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.137%\n",
      "\n",
      "\n",
      "\tNo. epoch: 136/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.271%\n",
      "\n",
      "\n",
      "\tNo. epoch: 137/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.204%\n",
      "\n",
      "\n",
      "\tNo. epoch: 138/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.162%\n",
      "\n",
      "\n",
      "\tNo. epoch: 139/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.264%\n",
      "\n",
      "\n",
      "\tNo. epoch: 140/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.258%\n",
      "\n",
      "\n",
      "\tNo. epoch: 141/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.325%\n",
      "\n",
      "\n",
      "\tNo. epoch: 142/200                   \n",
      "\t   Train Loss: 0.229 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.143%\n",
      "\n",
      "\n",
      "\tNo. epoch: 143/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.981%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.131%\n",
      "\n",
      "\n",
      "\tNo. epoch: 144/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.979%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.179%\n",
      "\n",
      "\n",
      "\tNo. epoch: 145/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.191%\n",
      "\n",
      "\n",
      "\tNo. epoch: 146/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.201%\n",
      "\n",
      "\n",
      "\tNo. epoch: 147/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.992%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.21%\n",
      "\n",
      "\n",
      "\tNo. epoch: 148/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.191%\n",
      "\n",
      "\n",
      "\tNo. epoch: 149/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.325%\n",
      "\n",
      "\n",
      "\tNo. epoch: 150/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.319%\n",
      "\n",
      "\n",
      "\tNo. epoch: 151/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.331%\n",
      "\n",
      "\n",
      "\tNo. epoch: 152/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.312%\n",
      "\n",
      "\n",
      "\tNo. epoch: 153/200                   \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.307%\n",
      "\t    Val. Loss: 0.153 |  Val. Acc: 97.358%\n",
      "\n",
      "\n",
      "\tNo. epoch: 154/200                   \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.374%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.204%\n",
      "\n",
      "\n",
      "\tNo. epoch: 155/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.975%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.226%\n",
      "\n",
      "\n",
      "\tNo. epoch: 156/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.306%\n",
      "\n",
      "\n",
      "\tNo. epoch: 157/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.245%\n",
      "\n",
      "\n",
      "\tNo. epoch: 158/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.233%\n",
      "\n",
      "\n",
      "\tNo. epoch: 159/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.245%\n",
      "\n",
      "\n",
      "\tNo. epoch: 160/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.312%\n",
      "\n",
      "\n",
      "\tNo. epoch: 161/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.306%\n",
      "\n",
      "\n",
      "\tNo. epoch: 162/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.373%\n",
      "\n",
      "\n",
      "\tNo. epoch: 163/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.264%\n",
      "\n",
      "\n",
      "\tNo. epoch: 164/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.258%\n",
      "\n",
      "\n",
      "\tNo. epoch: 165/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.978%\n",
      "\t    Val. Loss: 0.153 |  Val. Acc: 98.146%\n",
      "\n",
      "\n",
      "\tNo. epoch: 166/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.917%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.262%\n",
      "\n",
      "\n",
      "\tNo. epoch: 167/200                   \n",
      "\t   Train Loss: 0.232 | Train Acc: 98.864%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.648%\n",
      "\n",
      "\n",
      "\tNo. epoch: 168/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.892%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.354%\n",
      "\n",
      "\n",
      "\tNo. epoch: 169/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.992%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.408%\n",
      "\n",
      "\n",
      "\tNo. epoch: 170/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.414%\n",
      "\n",
      "\n",
      "\tNo. epoch: 171/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.395%\n",
      "\n",
      "\n",
      "\tNo. epoch: 172/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.414%\n",
      "\n",
      "\n",
      "\tNo. epoch: 173/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.402%\n",
      "\n",
      "\n",
      "\tNo. epoch: 174/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.408%\n",
      "\n",
      "\n",
      "\tNo. epoch: 175/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.354%\n",
      "\n",
      "\n",
      "\tNo. epoch: 176/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.414%\n",
      "\n",
      "\n",
      "\tNo. epoch: 177/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.408%\n",
      "\n",
      "\n",
      "\tNo. epoch: 178/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.402%\n",
      "\n",
      "\n",
      "\tNo. epoch: 179/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.408%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNo. epoch: 180/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.379%\n",
      "\n",
      "\n",
      "\tNo. epoch: 181/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.983%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.262%\n",
      "\n",
      "\n",
      "\tNo. epoch: 182/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.992%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.36%\n",
      "\n",
      "\n",
      "\tNo. epoch: 183/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.245%\n",
      "\n",
      "\n",
      "\tNo. epoch: 184/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.99%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.245%\n",
      "\n",
      "\n",
      "\tNo. epoch: 185/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.3%\n",
      "\n",
      "\n",
      "\tNo. epoch: 186/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.347%\n",
      "\n",
      "\n",
      "\tNo. epoch: 187/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.274%\n",
      "\n",
      "\n",
      "\tNo. epoch: 188/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.306%\n",
      "\n",
      "\n",
      "\tNo. epoch: 189/200                   \n",
      "\t   Train Loss: 0.231 | Train Acc: 99.559%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 98.978%\n",
      "\n",
      "\n",
      "\tNo. epoch: 190/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.905%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.316%\n",
      "\n",
      "\n",
      "\tNo. epoch: 191/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.992%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.284%\n",
      "\n",
      "\n",
      "\tNo. epoch: 192/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.997%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.344%\n",
      "\n",
      "\n",
      "\tNo. epoch: 193/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 99.999%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.386%\n",
      "\n",
      "\n",
      "\tNo. epoch: 194/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.151 |  Val. Acc: 99.35%\n",
      "\n",
      "\n",
      "\tNo. epoch: 195/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.369%\n",
      "\n",
      "\n",
      "\tNo. epoch: 196/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.357%\n",
      "\n",
      "\n",
      "\tNo. epoch: 197/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.363%\n",
      "\n",
      "\n",
      "\tNo. epoch: 198/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.357%\n",
      "\n",
      "\n",
      "\tNo. epoch: 199/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.344%\n",
      "\n",
      "\n",
      "\tNo. epoch: 200/200                   \n",
      "\t   Train Loss: 0.23 | Train Acc: 100.0%\n",
      "\t    Val. Loss: 0.15 |  Val. Acc: 99.223%\n",
      "\n",
      "\n",
      "Wall time: 40min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set seed.\n",
    "torch.manual_seed(1997)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# instantiate the model\n",
    "l4_MultiheadAttentionClassifier = MultiheadAttentionClassifier(\n",
    "    n_classes=1,\n",
    "    num_embeddings=len(TEXT.vocab),\n",
    "    embedding_dim=256,\n",
    "    num_layers=2,\n",
    "    heads=8,\n",
    "    device=\"cuda\",\n",
    "    augmentation_factor=4,\n",
    "    dropout=0,\n",
    "    max_length=fix_length,\n",
    "    pad_idx=TEXT.vocab.stoi[TEXT.pad_token]).to(\"cuda\")\n",
    "\n",
    "print(l4_MultiheadAttentionClassifier, \"\\n\")\n",
    "print(f'The model has {count_parameters(l4_MultiheadAttentionClassifier):,} trainable parameters.', \"\\n\")\n",
    "\n",
    "# Let us use Adam.\n",
    "OPT = optim.Adam(l4_MultiheadAttentionClassifier.parameters(), lr=0.0001)\n",
    "\n",
    "# Specify criterion.\n",
    "CRIT = nn.PoissonNLLLoss()\n",
    "\n",
    "MultiheadAttentionClassifier_train(l4_MultiheadAttentionClassifier,\n",
    "                                   train_iterator,\n",
    "                                   valid_iterator,\n",
    "                                   n_epochs,\n",
    "                                   OPT,\n",
    "                                   CRIT,\n",
    "                                   accuracy_function=poisson_accuracy,\n",
    "                                   save=True,\n",
    "                                   saving_path=\"model_bck/l4_MultiheadAttentionClassifier_lr0.0001_B384_D0_GC_false.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f228bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"model_bck/l1_MultiheadAttentionClassifier.pt\")\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
